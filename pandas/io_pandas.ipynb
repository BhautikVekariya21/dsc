{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65033630",
   "metadata": {},
   "source": [
    "# üìä Working with CSV Files in Pandas\n",
    "\n",
    "## üì• Reading CSV Files with `read_csv()`\n",
    "\n",
    "The `pandas.read_csv()` function is a powerful tool for reading CSV (Comma-Separated Values) files into a Pandas DataFrame, a versatile two-dimensional tabular data structure. This guide explains the most useful and frequently used parameters of `read_csv()` with examples using a real-world dataset, focusing on practical applications for data analysis.\n",
    "\n",
    "### üîπ Dataset\n",
    "We‚Äôll use the **Titanic Dataset**, which contains passenger data from the Titanic, including columns like `PassengerId`, `Survived`, `Pclass`, `Name`, `Sex`, `Age`, and more. The dataset is available at:\n",
    "- URL: `https://raw.githubusercontent.com/datasciencedojo/datasets/master/Titanic.csv`\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Most Useful `read_csv()` Parameters\n",
    "\n",
    "Below are the most useful and frequently used parameters for `read_csv()`, selected from the provided function signature, along with their descriptions and use cases.\n",
    "\n",
    "| Parameter          | Description                                              |\n",
    "|--------------------|----------------------------------------------------------|\n",
    "| `filepath_or_buffer` | Path to the CSV file, URL, or file-like object. Required to specify the data source. |\n",
    "| `sep`             | Delimiter used in the file (e.g., `','`, `';'`, `'\\t'`). Default is `','`. Use for non-standard delimiters. |\n",
    "| `header`          | Row number (0-based) to use as column names or `'infer'` to detect automatically. Set to `None` if no header exists. |\n",
    "| `names`           | List of column names to use when `header=None`. Overrides the file‚Äôs header row. |\n",
    "| `index_col`       | Column name or index to set as the DataFrame‚Äôs index. Useful for unique identifiers like `PassengerId`. |\n",
    "| `usecols`         | Columns to read, specified by names or indices. Reduces memory usage by selecting specific columns. |\n",
    "| `dtype`           | Dictionary specifying data types for columns (e.g., `{'Age': float}`). Ensures correct data type handling. |\n",
    "| `skiprows`        | Number of rows to skip at the start or a list of specific row indices. Useful for skipping metadata or headers. |\n",
    "| `skipfooter`      | Number of rows to skip at the end of the file. Useful for files with footer metadata or summaries. |\n",
    "| `nrows`           | Number of rows to read. Ideal for previewing large datasets or limiting data import. |\n",
    "| `na_values`       | Additional strings to recognize as NA/NaN (e.g., `['N/A', 'Missing']`). Customizes missing value detection. |\n",
    "| `parse_dates`     | List of column names to parse as datetime objects. Useful for date columns (not applicable in Titanic dataset). |\n",
    "| `encoding`        | Encoding to use for the file (e.g., `'utf-8'`, `'latin1'`). Handles files with special characters. |\n",
    "| `compression`     | Compression method for the file (e.g., `'gzip'`, `'zip'`). Default is `'infer'` to detect automatically. |\n",
    "| `on_bad_lines`    | Action for handling bad lines: `'error'`, `'warn'`, or `'skip'`. Useful for malformed CSV files. |\n",
    "| `chunksize`       | Number of rows to read per chunk for iterative processing. Useful for large files to manage memory. |\n",
    "| `comment`         | Character indicating a comment line to skip (e.g., `'#'`). Ignores lines starting with the specified character. |\n",
    "| `thousands`       | Character used as thousands separator (e.g., `','` for `1,000`). Ensures correct numeric parsing. |\n",
    "| `decimal`         | Character used as decimal point (e.g., `'.'` or `','`). Common in European datasets with comma decimals. |\n",
    "| `quotechar`       | Character used to quote fields (e.g., `'\"'`). Handles fields with delimiters or special characters. |\n",
    "| `doublequote`     | If `True`, two consecutive `quotechar` instances within a quoted field are treated as a single character. Default is `True`. |\n",
    "\n",
    "For a complete list of parameters, refer to the [official pandas documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html).\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Examples\n",
    "\n",
    "Below are examples of how to use each parameter with the Titanic dataset. These are implemented in a separate Python script (`csv_pandas_examples.py`).\n",
    "\n",
    "1. **Basic Usage (`filepath_or_buffer`)**: Reads the CSV file from a URL into a DataFrame using default settings.\n",
    "2. **Specifying Separator (`sep`)**: Defines a custom delimiter if the CSV uses a non-comma separator (Titanic uses commas, shown for illustration).\n",
    "3. **Setting Header (`header`)**: Uses a specific row as column names or skips headers.\n",
    "4. **Custom Column Names (`names`)**: Defines custom column names when the file lacks headers or to override existing ones.\n",
    "5. **Setting Index (`index_col`)**: Sets a column (e.g., `PassengerId`) as the DataFrame index.\n",
    "6. **Selecting Columns (`usecols`)**: Reads only specific columns to focus analysis and save memory.\n",
    "7. **Specifying Data Types (`dtype`)**: Enforces specific data types for columns like `Age` and `Fare`.\n",
    "8. **Skipping Rows (`skiprows`)**: Skips initial rows, useful for files with metadata or titles.\n",
    "9. **Skipping Footer (`skipfooter`)**: Skips rows at the end of the file, useful for files with summary rows.\n",
    "10. **Limiting Rows (`nrows`)**: Reads only a specified number of rows for quick previews.\n",
    "11. **Handling Missing Values (`na_values`)**: Treats custom strings as missing values (NaN).\n",
    "12. **Parsing Dates (`parse_dates`)**: Converts date columns to datetime format (not applicable in Titanic dataset, shown for completeness).\n",
    "13. **Specifying Encoding (`encoding`)**: Handles files with special characters (e.g., non-UTF-8 encoding).\n",
    "14. **Handling Compression (`compression`)**: Reads compressed CSV files (e.g., `.gz` files, shown for illustration).\n",
    "15. **Handling Bad Lines (`on_bad_lines`)**: Skips or warns about malformed rows in the CSV.\n",
    "16. **Reading in Chunks (`chunksize`)**: Processes large files in chunks to manage memory.\n",
    "17. **Skipping Comments (`comment`)**: Ignores lines starting with a specified character (e.g., `'#'`).\n",
    "18. **Handling Thousands Separator (`thousands`)**: Parses numbers with thousands separators (e.g., `1,000`).\n",
    "19. **Handling Decimal Point (`decimal`)**: Parses numbers with custom decimal points (e.g., `1,23` in European formats).\n",
    "20. **Handling Quoted Fields (`quotechar`)**: Specifies the character used to quote fields.\n",
    "21. **Handling Double Quotes (`doublequote`)**: Treats consecutive quote characters as a single character within quoted fields.\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Most Popular Parameters\n",
    "\n",
    "The following `read_csv()` parameters are the most commonly used in data analysis due to their versatility and frequent applicability:\n",
    "- **`filepath_or_buffer`**: Always required to specify the file or URL.\n",
    "- **`sep`**: Essential for CSVs with non-standard delimiters (e.g., semicolons, tabs).\n",
    "- **`header`**: Often adjusted for files with metadata or non-standard headers.\n",
    "- **`usecols`**: Widely used to select relevant columns, reducing memory usage.\n",
    "- **`parse_dates`**: Critical for time-series data to ensure proper datetime formatting.\n",
    "- **`skiprows`**: Common for skipping introductory text or metadata.\n",
    "- **`nrows`**: Useful for previewing large datasets or limiting data import.\n",
    "- **`na_values`**: Frequently used to handle custom missing value indicators.\n",
    "- **`encoding`**: Important for files with non-standard encodings, especially in international datasets.\n",
    "- **`chunksize`**: Essential for processing large files efficiently.\n",
    "- **`on_bad_lines`**: Commonly used to handle malformed CSVs gracefully.\n",
    "- **`thousands`**: Useful for datasets with formatted numbers (e.g., `1,000`).\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Notes\n",
    "- **Dependencies**: Requires `pandas`. Install with:\n",
    "  ```bash\n",
    "  pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebe254f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### üìù Python Script: `csv_pandas_examples.py`\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# URL for the Titanic dataset\n",
    "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/Titanic.csv'\n",
    "\n",
    "# üîπ Example 1: Basic Usage (filepath_or_buffer)\n",
    "df = pd.read_csv(url)\n",
    "print(\"Example 1: Basic Usage (First 5 rows):\")\n",
    "print(df.head(), '\\n')\n",
    "\n",
    "# üîπ Example 2: Specifying separator (sep)\n",
    "# Titanic uses commas, but demonstrating with default for completeness\n",
    "df_sep = pd.read_csv(url, sep=',')\n",
    "print(\"Example 2: Reading with comma separator (sep=','):\")\n",
    "print(df_sep.head(), '\\n')\n",
    "\n",
    "# üîπ Example 3: Specifying header\n",
    "# Using the second row as header (assuming first row is metadata)\n",
    "df_header = pd.read_csv(url, header=1)\n",
    "print(\"Example 3: Using second row as header (header=1):\")\n",
    "print(df_header.head(), '\\n')\n",
    "\n",
    "# üîπ Example 4: Custom column names (names)\n",
    "custom_columns = ['ID', 'Survived', 'Class', 'Name', 'Gender', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n",
    "df_names = pd.read_csv(url, header=None, names=custom_columns)\n",
    "print(\"Example 4: Custom column names (names):\")\n",
    "print(df_names.head(), '\\n')\n",
    "\n",
    "# üîπ Example 5: Setting index_col\n",
    "df_index = pd.read_csv(url, index_col='PassengerId')\n",
    "print(\"Example 5: Setting 'PassengerId' as index (index_col):\")\n",
    "print(df_index.head(), '\\n')\n",
    "\n",
    "# üîπ Example 6: Selecting specific columns (usecols)\n",
    "df_usecols = pd.read_csv(url, usecols=['PassengerId', 'Name', 'Age', 'Fare'])\n",
    "print(\"Example 6: Reading specific columns (usecols):\")\n",
    "print(df_usecols.head(), '\\n')\n",
    "\n",
    "# üîπ Example 7: Specifying data types (dtype)\n",
    "df_dtype = pd.read_csv(url, dtype={'Age': float, 'Fare': float})\n",
    "print(\"Example 7: Specifying data types (dtype):\")\n",
    "print(df_dtype.dtypes, '\\n')\n",
    "\n",
    "# üîπ Example 8: Skipping rows (skiprows)\n",
    "df_skiprows = pd.read_csv(url, skiprows=2)\n",
    "print(\"Example 8: Skipping first 2 rows (skiprows):\")\n",
    "print(df_skiprows.head(), '\\n')\n",
    "\n",
    "# üîπ Example 9: Skipping footer (skipfooter)\n",
    "# Titanic dataset has no footer, but demonstrating for completeness\n",
    "df_skipfooter = pd.read_csv(url, skipfooter=1, engine='python')\n",
    "print(\"Example 9: Skipping last row (skipfooter=1):\")\n",
    "print(df_skipfooter.tail(), '\\n')\n",
    "\n",
    "# üîπ Example 10: Reading limited rows (nrows)\n",
    "df_nrows = pd.read_csv(url, nrows=100)\n",
    "print(\"Example 10: Reading only 100 rows (nrows):\")\n",
    "print(df_nrows.shape, '\\n')\n",
    "\n",
    "# üîπ Example 11: Handling missing values (na_values)\n",
    "df_na = pd.read_csv(url, na_values=['N/A', 'Missing'])\n",
    "print(\"Example 11: Treating 'N/A' and 'Missing' as NaN (na_values):\")\n",
    "print(df_na.isna().sum(), '\\n')\n",
    "\n",
    "# üîπ Example 12: Parsing dates (parse_dates)\n",
    "# Titanic dataset has no date columns, included for completeness\n",
    "df_dates = pd.read_csv(url)  # No date column in Titanic dataset\n",
    "print(\"Example 12: Parsing dates (parse_dates, not applicable in Titanic dataset):\")\n",
    "print(df_dates.dtypes, '\\n')\n",
    "\n",
    "# üîπ Example 13: Specifying encoding (encoding)\n",
    "# Titanic dataset uses UTF-8, but demonstrating with 'latin1' for illustration\n",
    "df_encoding = pd.read_csv(url, encoding='latin1')\n",
    "print(\"Example 13: Specifying encoding (encoding='latin1'):\")\n",
    "print(df_encoding.head(), '\\n')\n",
    "\n",
    "# üîπ Example 14: Handling compression (compression)\n",
    "# Titanic dataset is uncompressed, but demonstrating with 'infer' for illustration\n",
    "df_compression = pd.read_csv(url, compression='infer')\n",
    "print(\"Example 14: Handling compression (compression='infer'):\")\n",
    "print(df_compression.head(), '\\n')\n",
    "\n",
    "# üîπ Example 15: Handling bad lines (on_bad_lines)\n",
    "# Titanic dataset is clean, but demonstrating with 'skip' for illustration\n",
    "df_bad_lines = pd.read_csv(url, on_bad_lines='skip')\n",
    "print(\"Example 15: Handling bad lines (on_bad_lines='skip'):\")\n",
    "print(df_bad_lines.head(), '\\n')\n",
    "\n",
    "# üîπ Example 16: Reading in chunks (chunksize)\n",
    "# Process the file in chunks of 100 rows\n",
    "chunk_iter = pd.read_csv(url, chunksize=100)\n",
    "print(\"Example 16: Reading in chunks (chunksize=100):\")\n",
    "for i, chunk in enumerate(chunk_iter):\n",
    "    print(f\"Chunk {i+1} (First 2 rows):\")\n",
    "    print(chunk.head(2))\n",
    "    if i == 1:  # Limit to 2 chunks for brevity\n",
    "        break\n",
    "print('\\n')\n",
    "\n",
    "# üîπ Example 17: Skipping comments (comment)\n",
    "# Titanic dataset has no comments, but demonstrating with '#' for illustration\n",
    "df_comment = pd.read_csv(url, comment='#')\n",
    "print(\"Example 17: Skipping comment lines (comment='#'):\")\n",
    "print(df_comment.head(), '\\n')\n",
    "\n",
    "# üîπ Example 18: Handling thousands separator (thousands)\n",
    "# Titanic dataset has no thousands separators, but demonstrating with ',' for illustration\n",
    "df_thousands = pd.read_csv(url, thousands=',')\n",
    "print(\"Example 18: Handling thousands separator (thousands=','):\")\n",
    "print(df_thousands[['Fare']].head(), '\\n')\n",
    "\n",
    "# üîπ Example 19: Handling decimal point (decimal)\n",
    "# Titanic dataset uses '.', but demonstrating with ',' for illustration\n",
    "df_decimal = pd.read_csv(url, decimal='.')\n",
    "print(\"Example 19: Handling decimal point (decimal='.'):\")\n",
    "print(df_decimal[['Fare']].head(), '\\n')\n",
    "\n",
    "# üîπ Example 20: Handling quoted fields (quotechar)\n",
    "# Titanic dataset uses standard quotes, but demonstrating with '\"' for illustration\n",
    "df_quotechar = pd.read_csv(url, quotechar='\"')\n",
    "print(\"Example 20: Handling quoted fields (quotechar='\\\"'):\")\n",
    "print(df_quotechar.head(), '\\n')\n",
    "\n",
    "# üîπ Example 21: Handling double quotes (doublequote)\n",
    "# Demonstrating with doublequote=True (default behavior)\n",
    "df_doublequote = pd.read_csv(url, doublequote=True)\n",
    "print(\"Example 21: Handling double quotes (doublequote=True):\")\n",
    "print(df_doublequote.head(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbab08a9",
   "metadata": {},
   "source": [
    "# üìä Working with Excel Files in Pandas\n",
    "\n",
    "## üì• Reading Excel Files with `read_excel()`\n",
    "\n",
    "The `pandas.read_excel()` function is used to read Excel files (`.xls`, `.xlsx`, `.xlsm`, etc.) into a Pandas DataFrame, a powerful two-dimensional tabular data structure. This guide explains the most useful and frequently used parameters of `read_excel()` with examples using a real-world dataset, focusing on practical applications for data analysis.\n",
    "\n",
    "### üîπ Dataset\n",
    "We‚Äôll use the **Sample Superstore** dataset, which contains sales data with columns such as `Order ID`, `Order Date`, `Customer Name`, `Sales`, `Profit`, and more. The dataset is available at:\n",
    "- URL: `https://github.com/vincentarelbundock/Rdatasets/raw/master/xlsx/Superstore.xlsx`\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Most Useful `read_excel()` Parameters\n",
    "\n",
    "Below are the most useful and frequently used parameters for `read_excel()`, including additional parameters that are valuable in real-world scenarios, along with their descriptions and use cases.\n",
    "\n",
    "| Parameter          | Description                                              |\n",
    "|--------------------|----------------------------------------------------------|\n",
    "| `io`              | Path to the Excel file, URL, or file-like object. Required to specify the data source. |\n",
    "| `sheet_name`      | Name (string), index (integer), or list of sheets to read. Default is `0` (first sheet). Useful for multi-sheet Excel files. |\n",
    "| `header`          | Row number (0-based) to use as column names. Default is `0`. Set to `None` if no header exists. |\n",
    "| `names`           | List of column names to use when `header=None`. Overrides the file‚Äôs header row. |\n",
    "| `index_col`       | Column name or index to set as the DataFrame‚Äôs index. Useful for unique identifiers like `Order ID`. |\n",
    "| `usecols`         | Columns to read, specified by names, indices, or Excel-style ranges (e.g., `'A:C'`). Reduces memory usage by selecting specific columns. |\n",
    "| `dtype`           | Dictionary specifying data types for columns (e.g., `{'Sales': float}`). Ensures correct data type handling. |\n",
    "| `skiprows`        | Number of rows to skip at the start or a list of specific row indices. Useful for skipping metadata or titles. |\n",
    "| `skipfooter`      | Number of rows to skip at the end of the file. Useful for files with summary rows or footers. |\n",
    "| `nrows`           | Number of rows to read. Ideal for previewing large datasets or limiting data import. |\n",
    "| `na_values`       | Additional strings to recognize as NA/NaN (e.g., `['N/A', 'Missing']`). Customizes missing value detection. |\n",
    "| `parse_dates`     | List of column names to parse as datetime objects. Essential for time-series analysis (e.g., `Order Date`). |\n",
    "| `converters`      | Dictionary of functions to apply to specific columns (e.g., `{'Sales': lambda x: float(x.replace('$', ''))}`). Useful for custom data transformations. |\n",
    "| `true_values`     | List of strings to recognize as `True` (e.g., `['Yes', 'TRUE']`). Useful for boolean columns. |\n",
    "| `false_values`    | List of strings to recognize as `False` (e.g., `['No', 'FALSE']`). Complements `true_values` for boolean handling. |\n",
    "| `engine`          | Excel engine to use (`'openpyxl'`, `'xlrd'`, or `'xlsxwriter'`). Default is `None` (auto-detect). Specify for compatibility or performance. |\n",
    "| `storage_options` | Dictionary of options for accessing remote files (e.g., authentication for cloud storage). Useful for files on S3, GCS, or other remote systems. |\n",
    "\n",
    "For a complete list of parameters, refer to the [official pandas documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html).\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Examples\n",
    "\n",
    "Below are examples of how to use each parameter with the Sample Superstore dataset. These are implemented in a separate Python script (`excel_pandas_examples.py`).\n",
    "\n",
    "1. **Basic Usage (`io`)**: Reads the Excel file from a URL into a DataFrame using default settings.\n",
    "2. **Specifying Sheet (`sheet_name`)**: Reads a specific sheet by index or name (default is the first sheet).\n",
    "3. **Setting Header (`header`)**: Uses a specific row as column names or skips headers.\n",
    "4. **Custom Column Names (`names`)**: Defines custom column names when the file lacks headers or to override existing ones.\n",
    "5. **Setting Index (`index_col`)**: Sets a column (e.g., `Order ID`) as the DataFrame index.\n",
    "6. **Selecting Columns (`usecols`)**: Reads only specific columns to focus analysis and save memory.\n",
    "7. **Specifying Data Types (`dtype`)**: Enforces specific data types for columns like `Sales` and `Profit`.\n",
    "8. **Skipping Rows (`skiprows`)**: Skips initial rows, useful for files with metadata or titles.\n",
    "9. **Skipping Footer (`skipfooter`)**: Skips rows at the end, useful for files with summary rows.\n",
    "10. **Limiting Rows (`nrows`)**: Reads only a specified number of rows for quick previews.\n",
    "11. **Handling Missing Values (`na_values`)**: Treats custom strings as missing values (NaN).\n",
    "12. **Parsing Dates (`parse_dates`)**: Converts columns like `Order Date` to datetime format.\n",
    "13. **Custom Converters (`converters`)**: Applies custom functions to columns for data transformation.\n",
    "14. **True Values (`true_values`)**: Recognizes specific strings as `True` for boolean columns.\n",
    "15. **False Values (`false_values`)**: Recognizes specific strings as `False` for boolean columns.\n",
    "16. **Specifying Engine (`engine`)**: Uses a specific Excel engine for compatibility or performance.\n",
    "17. **Storage Options (`storage_options`)**: Configures access to remote files (demonstrated for illustration).\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Most Popular Parameters\n",
    "\n",
    "The following `read_excel()` parameters are the most commonly used in real-world data analysis due to their versatility and frequent applicability:\n",
    "- **`io`**: Always required to specify the file or URL.\n",
    "- **`sheet_name`**: Essential for multi-sheet Excel files, common in business and financial datasets.\n",
    "- **`header`**: Often adjusted for files with metadata or non-standard headers.\n",
    "- **`usecols`**: Widely used to select relevant columns, reducing memory usage.\n",
    "- **`parse_dates`**: Critical for time-series data to ensure proper datetime formatting.\n",
    "- **`skiprows`**: Common for skipping introductory text or metadata.\n",
    "- **`nrows`**: Useful for previewing large datasets or limiting data import.\n",
    "- **`na_values`**: Frequently used to handle custom missing value indicators.\n",
    "- **`dtype`**: Ensures accurate data types, especially for numerical columns like `Sales`.\n",
    "- **`converters`**: Useful for custom data cleaning or transformation during import.\n",
    "- **`engine`**: Often specified to handle specific Excel file formats or optimize performance.\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Notes\n",
    "- **Dependencies**: Requires `pandas` and `openpyxl` (for `.xlsx` files) or `xlrd` (for older `.xls` files). Install with:\n",
    "  ```bash\n",
    "  pip install pandas openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7cc994",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### üìù Python Script: `excel_pandas_examples.py`\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# URL for the Sample Superstore dataset\n",
    "url = 'https://github.com/vincentarelbundock/Rdatasets/raw/master/xlsx/Superstore.xlsx'\n",
    "\n",
    "# üîπ Example 1: Basic Usage (io)\n",
    "df = pd.read_excel(url)\n",
    "print(\"Example 1: Basic Usage (First 5 rows):\")\n",
    "print(df.head(), '\\n')\n",
    "\n",
    "# üîπ Example 2: Specifying sheet_name\n",
    "df_sheet = pd.read_excel(url, sheet_name=0)\n",
    "print(\"Example 2: Reading specific sheet (sheet_name=0):\")\n",
    "print(df_sheet.head(), '\\n')\n",
    "\n",
    "# üîπ Example 3: Specifying header\n",
    "df_header = pd.read_excel(url, header=1)\n",
    "print(\"Example 3: Using second row as header (header=1):\")\n",
    "print(df_header.head(), '\\n')\n",
    "\n",
    "# üîπ Example 4: Custom column names (names)\n",
    "custom_columns = ['OrderID', 'OrderDate', 'Customer', 'Product', 'Sales', 'Profit']\n",
    "df_names = pd.read_excel(url, header=None, names=custom_columns)\n",
    "print(\"Example 4: Custom column names (names):\")\n",
    "print(df_names.head(), '\\n')\n",
    "\n",
    "# üîπ Example 5: Setting index_col\n",
    "df_index = pd.read_excel(url, index_col='Order ID')\n",
    "print(\"Example 5: Setting 'Order ID' as index (index_col):\")\n",
    "print(df_index.head(), '\\n')\n",
    "\n",
    "# üîπ Example 6: Selecting specific columns (usecols)\n",
    "df_usecols = pd.read_excel(url, usecols=['Order ID', 'Customer Name', 'Sales'])\n",
    "print(\"Example 6: Reading specific columns (usecols):\")\n",
    "print(df_usecols.head(), '\\n')\n",
    "\n",
    "# üîπ Example 7: Specifying data types (dtype)\n",
    "df_dtype = pd.read_excel(url, dtype={'Sales': float, 'Profit': float})\n",
    "print(\"Example 7: Specifying data types (dtype):\")\n",
    "print(df_dtype.dtypes, '\\n')\n",
    "\n",
    "# üîπ Example 8: Skipping rows (skiprows)\n",
    "df_skiprows = pd.read_excel(url, skiprows=2)\n",
    "print(\"Example 8: Skipping first 2 rows (skiprows):\")\n",
    "print(df_skiprows.head(), '\\n')\n",
    "\n",
    "# üîπ Example 9: Skipping footer (skipfooter)\n",
    "# Dataset may not have footer, but demonstrating for completeness\n",
    "df_skipfooter = pd.read_excel(url, skipfooter=1)\n",
    "print(\"Example 9: Skipping last row (skipfooter=1):\")\n",
    "print(df_skipfooter.tail(), '\\n')\n",
    "\n",
    "# üîπ Example 10: Reading limited rows (nrows)\n",
    "df_nrows = pd.read_excel(url, nrows=100)\n",
    "print(\"Example 10: Reading only 100 rows (nrows):\")\n",
    "print(df_nrows.shape, '\\n')\n",
    "\n",
    "# üîπ Example 11: Handling missing values (na_values)\n",
    "df_na = pd.read_excel(url, na_values=['N/A', 'Missing'])\n",
    "print(\"Example 11: Treating 'N/A' and 'Missing' as NaN (na_values):\")\n",
    "print(df_na.isna().sum(), '\\n')\n",
    "\n",
    "# üîπ Example 12: Parsing dates (parse_dates)\n",
    "df_dates = pd.read_excel(url, parse_dates=['Order Date'])\n",
    "print(\"Example 12: Parsing 'Order Date' as datetime (parse_dates):\")\n",
    "print(df_dates['Order Date'].dtype, '\\n')\n",
    "\n",
    "# üîπ Example 13: Custom converters (converters)\n",
    "# Example: Remove '$' from Sales if present and convert to float\n",
    "df_converters = pd.read_excel(url, converters={'Sales': lambda x: float(str(x).replace('$', ''))})\n",
    "print(\"Example 13: Applying custom converter to 'Sales' (converters):\")\n",
    "print(df_converters['Sales'].head(), '\\n')\n",
    "\n",
    "# üîπ Example 14: True values (true_values)\n",
    "# Dataset may not have boolean columns, but demonstrating for illustration\n",
    "df_true = pd.read_excel(url, true_values=['Yes', 'TRUE'])\n",
    "print(\"Example 14: Recognizing 'Yes' and 'TRUE' as True (true_values):\")\n",
    "print(df_true.head(), '\\n')\n",
    "\n",
    "# üîπ Example 15: False values (false_values)\n",
    "# Dataset may not have boolean columns, but demonstrating for illustration\n",
    "df_false = pd.read_excel(url, false_values=['No', 'FALSE'])\n",
    "print(\"Example 15: Recognizing 'No' and 'FALSE' as False (false_values):\")\n",
    "print(df_false.head(), '\\n')\n",
    "\n",
    "# üîπ Example 16: Specifying engine (engine)\n",
    "df_engine = pd.read_excel(url, engine='openpyxl')\n",
    "print(\"Example 16: Using openpyxl engine (engine='openpyxl'):\")\n",
    "print(df_engine.head(), '\\n')\n",
    "\n",
    "# üîπ Example 17: Storage options (storage_options)\n",
    "# Dataset is public, but demonstrating for remote storage (e.g., S3)\n",
    "df_storage = pd.read_excel(url, storage_options=None)  # None for public URL\n",
    "print(\"Example 17: Using storage options (storage_options=None):\")\n",
    "print(df_storage.head(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee24abad",
   "metadata": {},
   "source": [
    "# üìä Working with Text Files in Pandas\n",
    "\n",
    "## üì• Reading Text Files with `read_table()`\n",
    "\n",
    "The `pandas.read_table()` function is used to read text files (e.g., `.txt`, tab-delimited, or other delimited files) into a Pandas DataFrame, a versatile two-dimensional tabular data structure. It is similar to `pandas.read_csv()` but defaults to a tab (`'\\t'`) delimiter, making it ideal for tab-separated value (TSV) files or other delimited text files. This guide explains the most useful and frequently used parameters of `read_table()` with examples using a real-world dataset, focusing on practical applications for data analysis.\n",
    "\n",
    "### üîπ Dataset\n",
    "We‚Äôll use the **Iris Dataset**, which contains measurements of iris flowers, including columns like `sepal_length`, `sepal_width`, `petal_length`, `petal_width`, and `species`. The dataset is available as a tab-separated text file at:\n",
    "- URL: `https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.txt`\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Most Useful `read_table()` Parameters\n",
    "\n",
    "Below are the most useful and frequently used parameters for `read_table()`, adapted from the `pandas.read_csv()` signature (since `read_table()` shares most parameters) and tailored for text file handling, along with their descriptions and use cases.\n",
    "\n",
    "| Parameter          | Description                                              |\n",
    "|--------------------|----------------------------------------------------------|\n",
    "| `filepath_or_buffer` | Path to the text file, URL, or file-like object. Required to specify the data source. |\n",
    "| `sep`             | Delimiter used in the file (e.g., `'\\t'`, `','`, `';'`). Default is `'\\t'` for `read_table()`. Use for non-tab delimiters. |\n",
    "| `header`          | Row number (0-based) to use as column names or `'infer'` to detect automatically. Set to `None` if no header exists. |\n",
    "| `names`           | List of column names to use when `header=None`. Overrides the file‚Äôs header row. |\n",
    "| `index_col`       | Column name or index to set as the DataFrame‚Äôs index. Useful for unique identifiers or row indices. |\n",
    "| `usecols`         | Columns to read, specified by names or indices. Reduces memory usage by selecting specific columns. |\n",
    "| `dtype`           | Dictionary specifying data types for columns (e.g., `{'sepal_length': float}`). Ensures correct data type handling. |\n",
    "| `skiprows`        | Number of rows to skip at the start or a list of specific row indices. Useful for skipping metadata or headers. |\n",
    "| `skipfooter`      | Number of rows to skip at the end of the file. Useful for files with summary rows or footers. |\n",
    "| `nrows`           | Number of rows to read. Ideal for previewing large datasets or limiting data import. |\n",
    "| `na_values`       | Additional strings to recognize as NA/NaN (e.g., `['N/A', 'Missing']`). Customizes missing value detection. |\n",
    "| `parse_dates`     | List of column names to parse as datetime objects. Useful for date columns (not applicable in Iris dataset). |\n",
    "| `encoding`        | Encoding to use for the file (e.g., `'utf-8'`, `'latin1'`). Handles files with special characters. |\n",
    "| `compression`     | Compression method for the file (e.g., `'gzip'`, `'zip'`). Default is `'infer'` to detect automatically. |\n",
    "| `on_bad_lines`    | Action for handling bad lines: `'error'`, `'warn'`, or `'skip'`. Useful for malformed text files. |\n",
    "| `chunksize`       | Number of rows to read per chunk for iterative processing. Useful for large files to manage memory. |\n",
    "| `comment`         | Character indicating a comment line to skip (e.g., `'#'`). Ignores lines starting with the specified character. |\n",
    "| `thousands`       | Character used as thousands separator (e.g., `','` for `1,000`). Ensures correct numeric parsing. |\n",
    "| `decimal`         | Character used as decimal point (e.g., `'.'` or `','`). Common in datasets with non-standard decimal formats. |\n",
    "| `quotechar`       | Character used to quote fields (e.g., `'\"'`). Handles fields with delimiters or special characters. |\n",
    "| `doublequote`     | If `True`, two consecutive `quotechar` instances within a quoted field are treated as a single character. Default is `True`. |\n",
    "\n",
    "For a complete list of parameters, refer to the [official pandas documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_table.html).\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Examples\n",
    "\n",
    "Below are examples of how to use each parameter with the Iris dataset. These are implemented in a separate Python script (`text_pandas_examples.py`).\n",
    "\n",
    "1. **Basic Usage (`filepath_or_buffer`)**: Reads the text file from a URL into a DataFrame using default settings.\n",
    "2. **Specifying Separator (`sep`)**: Defines a custom delimiter (Iris uses tabs, shown for illustration).\n",
    "3. **Setting Header (`header`)**: Uses a specific row as column names or skips headers.\n",
    "4. **Custom Column Names (`names`)**: Defines custom column names when the file lacks headers or to override existing ones.\n",
    "5. **Setting Index (`index_col`)**: Sets a column as the DataFrame index (e.g., a row index).\n",
    "6. **Selecting Columns (`usecols`)**: Reads only specific columns to focus analysis and save memory.\n",
    "7. **Specifying Data Types (`dtype`)**: Enforces specific data types for columns like `sepal_length` and `petal_width`.\n",
    "8. **Skipping Rows (`skiprows`)**: Skips initial rows, useful for files with metadata or titles.\n",
    "9. **Skipping Footer (`skipfooter`)**: Skips rows at the end, useful for files with summary rows.\n",
    "10. **Limiting Rows (`nrows`)**: Reads only a specified number of rows for quick previews.\n",
    "11. **Handling Missing Values (`na_values`)**: Treats custom strings as missing values (NaN).\n",
    "12. **Parsing Dates (`parse_dates`)**: Converts date columns to datetime format (not applicable in Iris dataset, shown for completeness).\n",
    "13. **Specifying Encoding (`encoding`)**: Handles files with special characters (e.g., non-UTF-8 encoding).\n",
    "14. **Handling Compression (`compression`)**: Reads compressed text files (e.g., `.gz` files, shown for illustration).\n",
    "15. **Handling Bad Lines (`on_bad_lines`)**: Skips or warns about malformed rows in the text file.\n",
    "16. **Reading in Chunks (`chunksize`)**: Processes large files in chunks to manage memory.\n",
    "17. **Skipping Comments (`comment`)**: Ignores lines starting with a specified character (e.g., `'#'`).\n",
    "18. **Handling Thousands Separator (`thousands`)**: Parses numbers with thousands separators (e.g., `1,000`).\n",
    "19. **Handling Decimal Point (`decimal`)**: Parses numbers with custom decimal points (e.g., `1,23` in European formats).\n",
    "20. **Handling Quoted Fields (`quotechar`)**: Specifies the character used to quote fields.\n",
    "21. **Handling Double Quotes (`doublequote`)**: Treats consecutive quote characters as a single character within quoted fields.\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Most Popular Parameters\n",
    "\n",
    "The following `read_table()` parameters are the most commonly used in real-world data analysis due to their versatility and frequent applicability:\n",
    "- **`filepath_or_buffer`**: Always required to specify the file or URL.\n",
    "- **`sep`**: Essential for text files with non-tab delimiters (e.g., commas, semicolons).\n",
    "- **`header`**: Often adjusted for files with metadata or non-standard headers.\n",
    "- **`usecols`**: Widely used to select relevant columns, reducing memory usage.\n",
    "- **`parse_dates`**: Critical for time-series data to ensure proper datetime formatting.\n",
    "- **`skiprows`**: Common for skipping introductory text or metadata.\n",
    "- **`nrows`**: Useful for previewing large datasets or limiting data import.\n",
    "- **`na_values`**: Frequently used to handle custom missing value indicators.\n",
    "- **`encoding`**: Important for text files with non-standard encodings, especially in international datasets.\n",
    "- **`chunksize`**: Essential for processing large files efficiently.\n",
    "- **`on_bad_lines`**: Commonly used to handle malformed text files gracefully.\n",
    "- **`comment`**: Useful for skipping comment lines in scientific or log-based text files.\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Notes\n",
    "- **Dependencies**: Requires `pandas`. Install with:\n",
    "  ```bash\n",
    "  pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3723cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### üìù Python Script: `text_pandas_examples.py`\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# URL for the Iris dataset (tab-separated text file)\n",
    "url = 'https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.txt'\n",
    "\n",
    "# üîπ Example 1: Basic Usage (filepath_or_buffer)\n",
    "df = pd.read_table(url)\n",
    "print(\"Example 1: Basic Usage (First 5 rows):\")\n",
    "print(df.head(), '\\n')\n",
    "\n",
    "# üîπ Example 2: Specifying separator (sep)\n",
    "# Iris uses tabs, but demonstrating with '\\t' for completeness\n",
    "df_sep = pd.read_table(url, sep='\\t')\n",
    "print(\"Example 2: Reading with tab separator (sep='\\\\t'):\")\n",
    "print(df_sep.head(), '\\n')\n",
    "\n",
    "# üîπ Example 3: Specifying header\n",
    "# Using the second row as header (assuming first row is metadata)\n",
    "df_header = pd.read_table(url, header=1)\n",
    "print(\"Example 3: Using second row as header (header=1):\")\n",
    "print(df_header.head(), '\\n')\n",
    "\n",
    "# üîπ Example 4: Custom column names (names)\n",
    "custom_columns = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth', 'Species']\n",
    "df_names = pd.read_table(url, header=None, names=custom_columns)\n",
    "print(\"Example 4: Custom column names (names):\")\n",
    "print(df_names.head(), '\\n')\n",
    "\n",
    "# üîπ Example 5: Setting index_col\n",
    "# Using the first column as index (e.g., row number)\n",
    "df_index = pd.read_table(url, index_col=0)\n",
    "print(\"Example 5: Setting first column as index (index_col=0):\")\n",
    "print(df_index.head(), '\\n')\n",
    "\n",
    "# üîπ Example 6: Selecting specific columns (usecols)\n",
    "df_usecols = pd.read_table(url, usecols=['sepal_length', 'sepal_width', 'species'])\n",
    "print(\"Example 6: Reading specific columns (usecols):\")\n",
    "print(df_usecols.head(), '\\n')\n",
    "\n",
    "# üîπ Example 7: Specifying data types (dtype)\n",
    "df_dtype = pd.read_table(url, dtype={'sepal_length': float, 'sepal_width': float})\n",
    "print(\"Example 7: Specifying data types (dtype):\")\n",
    "print(df_dtype.dtypes, '\\n')\n",
    "\n",
    "# üîπ Example 8: Skipping rows (skiprows)\n",
    "df_skiprows = pd.read_table(url, skiprows=2)\n",
    "print(\"Example 8: Skipping first 2 rows (skiprows):\")\n",
    "print(df_skiprows.head(), '\\n')\n",
    "\n",
    "# üîπ Example 9: Skipping footer (skipfooter)\n",
    "# Iris dataset has no footer, but demonstrating for completeness\n",
    "df_skipfooter = pd.read_table(url, skipfooter=1, engine='python')\n",
    "print(\"Example 9: Skipping last row (skipfooter=1):\")\n",
    "print(df_skipfooter.tail(), '\\n')\n",
    "\n",
    "# üîπ Example 10: Reading limited rows (nrows)\n",
    "df_nrows = pd.read_table(url, nrows=50)\n",
    "print(\"Example 10: Reading only 50 rows (nrows):\")\n",
    "print(df_nrows.shape, '\\n')\n",
    "\n",
    "# üîπ Example 11: Handling missing values (na_values)\n",
    "df_na = pd.read_table(url, na_values=['N/A', 'Missing'])\n",
    "print(\"Example 11: Treating 'N/A' and 'Missing' as NaN (na_values):\")\n",
    "print(df_na.isna().sum(), '\\n')\n",
    "\n",
    "# üîπ Example 12: Parsing dates (parse_dates)\n",
    "# Iris dataset has no date columns, included for completeness\n",
    "df_dates = pd.read_table(url)\n",
    "print(\"Example 12: Parsing dates (parse_dates, not applicable in Iris dataset):\")\n",
    "print(df_dates.dtypes, '\\n')\n",
    "\n",
    "# üîπ Example 13: Specifying encoding (encoding)\n",
    "# Iris dataset uses UTF-8, but demonstrating with 'latin1' for illustration\n",
    "df_encoding = pd.read_table(url, encoding='latin1')\n",
    "print(\"Example 13: Specifying encoding (encoding='latin1'):\")\n",
    "print(df_encoding.head(), '\\n')\n",
    "\n",
    "# üîπ Example 14: Handling compression (compression)\n",
    "# Iris dataset is uncompressed, but demonstrating with 'infer' for illustration\n",
    "df_compression = pd.read_table(url, compression='infer')\n",
    "print(\"Example 14: Handling compression (compression='infer'):\")\n",
    "print(df_compression.head(), '\\n')\n",
    "\n",
    "# üîπ Example 15: Handling bad lines (on_bad_lines)\n",
    "# Iris dataset is clean, but demonstrating with 'skip' for illustration\n",
    "df_bad_lines = pd.read_table(url, on_bad_lines='skip')\n",
    "print(\"Example 15: Handling bad lines (on_bad_lines='skip'):\")\n",
    "print(df_bad_lines.head(), '\\n')\n",
    "\n",
    "# üîπ Example 16: Reading in chunks (chunksize)\n",
    "# Process the file in chunks of 50 rows\n",
    "chunk_iter = pd.read_table(url, chunksize=50)\n",
    "print(\"Example 16: Reading in chunks (chunksize=50):\")\n",
    "for i, chunk in enumerate(chunk_iter):\n",
    "    print(f\"Chunk {i+1} (First 2 rows):\")\n",
    "    print(chunk.head(2))\n",
    "    if i == 1:  # Limit to 2 chunks for brevity\n",
    "        break\n",
    "print('\\n')\n",
    "\n",
    "# üîπ Example 17: Skipping comments (comment)\n",
    "# Iris dataset has no comments, but demonstrating with '#' for illustration\n",
    "df_comment = pd.read_table(url, comment='#')\n",
    "print(\"Example 17: Skipping comment lines (comment='#'):\")\n",
    "print(df_comment.head(), '\\n')\n",
    "\n",
    "# üîπ Example 18: Handling thousands separator (thousands)\n",
    "# Iris dataset has no thousands separators, but demonstrating with ',' for illustration\n",
    "df_thousands = pd.read_table(url, thousands=',')\n",
    "print(\"Example 18: Handling thousands separator (thousands=','):\")\n",
    "print(df_thousands[['sepal_length']].head(), '\\n')\n",
    "\n",
    "# üîπ Example 19: Handling decimal point (decimal)\n",
    "# Iris dataset uses '.', but demonstrating with '.' for illustration\n",
    "df_decimal = pd.read_table(url, decimal='.')\n",
    "print(\"Example 19: Handling decimal point (decimal='.'):\")\n",
    "print(df_decimal[['sepal_length']].head(), '\\n')\n",
    "\n",
    "# üîπ Example 20: Handling quoted fields (quotechar)\n",
    "# Iris dataset uses no quotes, but demonstrating with '\"' for illustration\n",
    "df_quotechar = pd.read_table(url, quotechar='\"')\n",
    "print(\"Example 20: Handling quoted fields (quotechar='\\\"'):\")\n",
    "print(df_quotechar.head(), '\\n')\n",
    "\n",
    "# üîπ Example 21: Handling double quotes (doublequote)\n",
    "# Demonstrating with doublequote=True (default behavior)\n",
    "df_doublequote = pd.read_table(url, doublequote=True)\n",
    "print(\"Example 21: Handling double quotes (doublequote=True):\")\n",
    "print(df_doublequote.head(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2957b1",
   "metadata": {},
   "source": [
    "# üìä Working with JSON Files in Pandas\n",
    "\n",
    "## üì• Reading JSON Files with `read_json()`\n",
    "\n",
    "The `pandas.read_json()` function is used to read JSON (JavaScript Object Notation) files or strings into a Pandas DataFrame, a versatile two-dimensional tabular data structure. This function is ideal for handling structured JSON data, commonly used in APIs, web services, and data exchanges. This guide explains the most useful and frequently used parameters of `read_json()` with examples using a real-world dataset, focusing on practical applications for data analysis.\n",
    "\n",
    "### üîπ Dataset\n",
    "We‚Äôll use the **World Bank Population Data**, which contains population statistics for countries, with fields like `country`, `year`, `population`, etc. The dataset is available as a JSON file at:\n",
    "- URL: `https://raw.githubusercontent.com/nickmccullum/python-data-analysis/main/population_data.json`\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Most Useful `read_json()` Parameters\n",
    "\n",
    "Below are the most useful and frequently used parameters for `read_json()`, selected from the `pandas.read_json()` signature, along with their descriptions and use cases.\n",
    "\n",
    "| Parameter          | Description                                              |\n",
    "|--------------------|----------------------------------------------------------|\n",
    "| `path_or_buf`     | Path to the JSON file, URL, or JSON string. Required to specify the data source. |\n",
    "| `orient`          | Format of the JSON data: `'records'`, `'split'`, `'index'`, `'columns'`, or `'values'`. Default is `None` (inferred). Specifies the JSON structure. |\n",
    "| `typ`             | Type of object to return: `'frame'` (DataFrame) or `'series'`. Default is `'frame'`. Useful for specific output formats. |\n",
    "| `dtype`           | Dictionary specifying data types for columns (e.g., `{'population': int}`). Ensures correct data type handling. |\n",
    "| `lines`           | If `True`, reads JSON data as JSON Lines (one JSON object per line). Default is `False`. Useful for streaming large datasets. |\n",
    "| `encoding`        | Encoding to use for the file (e.g., `'utf-8'`, `'latin1'`). Default is `'utf-8'`. Handles files with special characters. |\n",
    "| `nrows`           | Number of rows to read. Ideal for previewing large datasets or limiting data import. |\n",
    "| `parse_dates`     | List of column names to parse as datetime objects. Useful for date columns like `year`. |\n",
    "| `chunksize`       | Number of rows to read per chunk when using `TextFileReader`. Useful for large files to manage memory (requires `lines=True`). |\n",
    "| `compression`     | Compression method for the file (e.g., `'gzip'`, `'zip'`). Default is `'infer'` to detect automatically. |\n",
    "| `converters`      | Dictionary of functions to apply to specific columns (e.g., `{'population': lambda x: int(x)}`). Useful for custom data transformations. |\n",
    "\n",
    "For a complete list of parameters, refer to the [official pandas documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_json.html).\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Examples\n",
    "\n",
    "Below are examples of how to use each parameter with the World Bank Population Data dataset. These are implemented in a separate Python script (`json_pandas_examples.py`).\n",
    "\n",
    "1. **Basic Usage (`path_or_buf`)**: Reads the JSON file from a URL into a DataFrame using default settings.\n",
    "2. **Specifying Orientation (`orient`)**: Defines the JSON structure (e.g., `'records'`) to match the dataset format.\n",
    "3. **Specifying Type (`typ`)**: Returns a DataFrame or Series based on the `typ` parameter.\n",
    "4. **Specifying Data Types (`dtype`)**: Enforces specific data types for columns like `population`.\n",
    "5. **Reading JSON Lines (`lines`)**: Reads JSON Lines format (demonstrated with a compatible dataset or modification).\n",
    "6. **Specifying Encoding (`encoding`)**: Handles files with special characters (e.g., non-UTF-8 encoding).\n",
    "7. **Limiting Rows (`nrows`)**: Reads only a specified number of rows for quick previews.\n",
    "8. **Parsing Dates (`parse_dates`)**: Converts columns like `year` to datetime format.\n",
    "9. **Reading in Chunks (`chunksize`)**: Processes large files in chunks to manage memory (requires `lines=True`).\n",
    "10. **Handling Compression (`compression`)**: Reads compressed JSON files (e.g., `.gz` files, shown for illustration).\n",
    "11. **Custom Converters (`converters`)**: Applies custom functions to columns for data transformation.\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Most Popular Parameters\n",
    "\n",
    "The following `read_json()` parameters are the most commonly used in real-world data analysis due to their versatility and frequent applicability:\n",
    "- **`path_or_buf`**: Always required to specify the file, URL, or JSON string.\n",
    "- **`orient`**: Essential for correctly interpreting JSON structures, especially for nested or non-standard JSON data.\n",
    "- **`lines`**: Critical for JSON Lines format, common in streaming or log-based datasets.\n",
    "- **`dtype`**: Ensures accurate data types, especially for numerical columns like `population`.\n",
    "- **`parse_dates`**: Important for time-series data to ensure proper datetime formatting.\n",
    "- **`nrows`**: Useful for previewing large datasets or limiting data import.\n",
    "- **`chunksize`**: Essential for processing large JSON files efficiently.\n",
    "- **`encoding`**: Frequently used for datasets with non-standard encodings, especially in international contexts.\n",
    "- **`converters`**: Useful for custom data cleaning or transformation during import.\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Notes\n",
    "- **Dependencies**: Requires `pandas`. Install with:\n",
    "  ```bash\n",
    "  pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0c5743",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### üìù Python Script: `json_pandas_examples.py`\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# URL for the World Bank Population Data (JSON format)\n",
    "url = 'https://raw.githubusercontent.com/nickmccullum/python-data-analysis/main/population_data.json'\n",
    "\n",
    "# üîπ Example 1: Basic Usage (path_or_buf)\n",
    "df = pd.read_json(url)\n",
    "print(\"Example 1: Basic Usage (First 5 rows):\")\n",
    "print(df.head(), '\\n')\n",
    "\n",
    "# üîπ Example 2: Specifying orientation (orient)\n",
    "# Dataset uses 'records' orientation, shown for confirmation\n",
    "df_orient = pd.read_json(url, orient='records')\n",
    "print(\"Example 2: Specifying orientation (orient='records'):\")\n",
    "print(df_orient.head(), '\\n')\n",
    "\n",
    "# üîπ Example 3: Specifying type (typ)\n",
    "# Return a Series instead of DataFrame for a single column\n",
    "df_typ = pd.read_json(url, typ='series', usecols=['population'])\n",
    "print(\"Example 3: Returning a Series (typ='series'):\")\n",
    "print(df_typ.head(), '\\n')\n",
    "\n",
    "# üîπ Example 4: Specifying data types (dtype)\n",
    "df_dtype = pd.read_json(url, dtype={'population': int})\n",
    "print(\"Example 4: Specifying data types (dtype):\")\n",
    "print(df_dtype.dtypes, '\\n')\n",
    "\n",
    "# üîπ Example 5: Reading JSON Lines (lines)\n",
    "# Dataset is not JSON Lines, but demonstrating for illustration\n",
    "# Note: This may raise an error if the dataset is not in JSON Lines format\n",
    "try:\n",
    "    df_lines = pd.read_json(url, lines=True)\n",
    "    print(\"Example 5: Reading JSON Lines (lines=True):\")\n",
    "    print(df_lines.head(), '\\n')\n",
    "except ValueError:\n",
    "    print(\"Example 5: JSON Lines not applicable for this dataset (lines=True)\\n\")\n",
    "\n",
    "# üîπ Example 6: Specifying encoding (encoding)\n",
    "# Dataset uses UTF-8, but demonstrating with 'latin1' for illustration\n",
    "df_encoding = pd.read_json(url, encoding='latin1')\n",
    "print(\"Example 6: Specifying encoding (encoding='latin1'):\")\n",
    "print(df_encoding.head(), '\\n')\n",
    "\n",
    "# üîπ Example 7: Reading limited rows (nrows)\n",
    "df_nrows = pd.read_json(url, nrows=50)\n",
    "print(\"Example 7: Reading only 50 rows (nrows):\")\n",
    "print(df_nrows.shape, '\\n')\n",
    "\n",
    "# üîπ Example 8: Parsing dates (parse_dates)\n",
    "df_dates = pd.read_json(url, parse_dates=['year'])\n",
    "print(\"Example 8: Parsing 'year' as datetime (parse_dates):\")\n",
    "print(df_dates['year'].dtype, '\\n')\n",
    "\n",
    "# üîπ Example 9: Reading in chunks (chunksize)\n",
    "# Requires lines=True; demonstrating with a JSON Lines-compatible dataset\n",
    "# Note: This may raise an error if the dataset is not in JSON Lines format\n",
    "try:\n",
    "    chunk_iter = pd.read_json(url, lines=True, chunksize=50)\n",
    "    print(\"Example 9: Reading in chunks (chunksize=50):\")\n",
    "    for i, chunk in enumerate(chunk_iter):\n",
    "        print(f\"Chunk {i+1} (First 2 rows):\")\n",
    "        print(chunk.head(2))\n",
    "        if i == 1:  # Limit to 2 chunks for brevity\n",
    "            break\n",
    "    print('\\n')\n",
    "except ValueError:\n",
    "    print(\"Example 9: Chunksize not applicable for non-JSON Lines dataset\\n\")\n",
    "\n",
    "# üîπ Example 10: Handling compression (compression)\n",
    "# Dataset is uncompressed, but demonstrating with 'infer' for illustration\n",
    "df_compression = pd.read_json(url, compression='infer')\n",
    "print(\"Example 10: Handling compression (compression='infer'):\")\n",
    "print(df_compression.head(), '\\n')\n",
    "\n",
    "# üîπ Example 11: Custom converters (converters)\n",
    "# Example: Convert population to integer\n",
    "df_converters = pd.read_json(url, converters={'population': lambda x: int(x)})\n",
    "print(\"Example 11: Applying custom converter to 'population' (converters):\")\n",
    "print(df_converters['population'].head(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32141d3",
   "metadata": {},
   "source": [
    "# üìä Working with MySQL Data in Pandas using MySQL Connector\n",
    "\n",
    "## üì• Reading MySQL Data with `read_sql_query()` and MySQL Connector\n",
    "\n",
    "The `pandas.read_sql_query()` function is used to read data from a MySQL database into a Pandas DataFrame, a powerful two-dimensional tabular data structure. When paired with **MySQL Connector/Python**, a Python driver for MySQL, it enables seamless interaction with MySQL databases. This guide explains how to connect to a MySQL database using `mysql.connector`, set up a MySQL server with **XAMPP**, and use the most useful parameters of `read_sql_query()` with examples using a real-world dataset.\n",
    "\n",
    "### üîπ Dataset\n",
    "We‚Äôll use the **Chinook Database** (MySQL version), a sample music store database with tables such as `customers`, `invoices`, `tracks`, etc., containing fields like `CustomerId`, `FirstName`, `LastName`, `InvoiceDate`, `Total`, etc. The MySQL version is available from:\n",
    "- Source: [Chinook Database MySQL](https://github.com/lerocha/chinook-database/raw/master/ChinookDatabase/DataSources/Chinook_MySql.sql)\n",
    "- Instructions: Download the SQL script and import it into your MySQL database using XAMPP‚Äôs phpMyAdmin or MySQL Workbench.\n",
    "\n",
    "### üîπ Setting Up MySQL with XAMPP\n",
    "\n",
    "**XAMPP** is a free, open-source software stack that includes Apache, MySQL, PHP, and phpMyAdmin, making it easy to set up a local MySQL server for development.\n",
    "\n",
    "#### Installation Steps\n",
    "1. **Download XAMPP**:\n",
    "   - Visit [Apache Friends](https://www.apachefriends.org/index.html) and download XAMPP for your operating system (Windows, macOS, or Linux).\n",
    "   - Verified as accessible on June 4, 2025, at 2:57 PM IST.\n",
    "2. **Install XAMPP**:\n",
    "   - Run the installer and select components: Apache, MySQL, and phpMyAdmin.\n",
    "   - Install to a default location (e.g., `C:\\xampp` on Windows).\n",
    "3. **Start MySQL**:\n",
    "   - Open the XAMPP Control Panel.\n",
    "   - Start the **Apache** and **MySQL** modules.\n",
    "   - MySQL typically runs on port `3306` (default).\n",
    "4. **Access phpMyAdmin**:\n",
    "   - Open a browser and navigate to `http://localhost/phpmyadmin`.\n",
    "   - Log in with the default credentials (username: `root`, password: usually empty unless changed).\n",
    "5. **Create the Chinook Database**:\n",
    "   - In phpMyAdmin, click **New** to create a database named `Chinook`.\n",
    "   - Go to the **Import** tab, upload the `Chinook_MySql.sql` file, and click **Go** to populate the database.\n",
    "   - Alternatively, use MySQL Workbench or the MySQL command line:\n",
    "     ```bash\n",
    "     mysql -u root -p < Chinook_MySql.sql\n",
    "     ```\n",
    "\n",
    "#### XAMPP Configuration Notes\n",
    "- **Default Credentials**: Username is `root`, password is empty by default. Set a password for security in production environments.\n",
    "- **Port Conflicts**: Ensure port `3306` is free. Check the XAMPP Control Panel or MySQL logs if MySQL fails to start.\n",
    "- **Accessing MySQL**: Use `localhost` or `127.0.0.1` as the host for local connections.\n",
    "- **Security**: For production, configure user permissions and enable a firewall to restrict database access.\n",
    "\n",
    "### üîπ Connecting to MySQL with MySQL Connector/Python\n",
    "\n",
    "**MySQL Connector/Python** is a pure Python driver for MySQL, allowing Python applications to connect to MySQL databases without additional dependencies.\n",
    "\n",
    "#### Installation\n",
    "Install the connector using pip:\n",
    "```bash\n",
    "pip install mysql-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4d5e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### üìù Python Script: `mysql_pandas_examples.py`\n",
    "\n",
    "import pandas as pd\n",
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "\n",
    "# MySQL connection configuration (update as needed)\n",
    "config = {\n",
    "    'host': 'localhost',\n",
    "    'user': 'root',\n",
    "    'password': '',  # Update if you set a password in XAMPP\n",
    "    'database': 'Chinook'\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Establish MySQL connection\n",
    "    conn = mysql.connector.connect(**config)\n",
    "\n",
    "    # üîπ Example 1: Basic Usage (sql, con)\n",
    "    df = pd.read_sql_query(sql='SELECT * FROM invoices', con=conn)\n",
    "    print(\"Example 1: Basic Usage (First 5 rows):\")\n",
    "    print(df.head(), '\\n')\n",
    "\n",
    "    # üîπ Example 2: Setting index_col\n",
    "    df_index = pd.read_sql_query(sql='SELECT * FROM invoices', con=conn, index_col='InvoiceId')\n",
    "    print(\"Example 2: Setting 'InvoiceId' as index (index_col):\")\n",
    "    print(df_index.head(), '\\n')\n",
    "\n",
    "    # üîπ Example 3: Specifying data types (dtype)\n",
    "    df_dtype = pd.read_sql_query(sql='SELECT * FROM invoices', con=conn, dtype={'Total': float})\n",
    "    print(\"Example 3: Specifying data types (dtype):\")\n",
    "    print(df_dtype.dtypes, '\\n')\n",
    "\n",
    "    # üîπ Example 4: Parsing dates (parse_dates)\n",
    "    df_dates = pd.read_sql_query(sql='SELECT * FROM invoices', con=conn, parse_dates=['InvoiceDate'])\n",
    "    print(\"Example 4: Parsing 'InvoiceDate' as datetime (parse_dates):\")\n",
    "    print(df_dates['InvoiceDate'].dtype, '\\n')\n",
    "\n",
    "    # üîπ Example 5: Reading in chunks (chunksize)\n",
    "    chunk_iter = pd.read_sql_query(sql='SELECT * FROM invoices', con=conn, chunksize=100)\n",
    "    print(\"Example 5: Reading in chunks (chunksize=100):\")\n",
    "    for i, chunk in enumerate(chunk_iter):\n",
    "        print(f\"Chunk {i+1} (First 2 rows):\")\n",
    "        print(chunk.head(2))\n",
    "        if i == 1:  # Limit to 2 chunks for brevity\n",
    "            break\n",
    "    print('\\n')\n",
    "\n",
    "    # üîπ Example 6: Using parameters (params)\n",
    "    # Parameterized query to filter invoices for a specific customer\n",
    "    df_params = pd.read_sql_query(sql='SELECT * FROM invoices WHERE CustomerId = %s', con=conn, params=(1,))\n",
    "    print(\"Example 6: Using parameterized query (params):\")\n",
    "    print(df_params.head(), '\\n')\n",
    "\n",
    "    # üîπ Example 7: Coercing floats (coerce_float)\n",
    "    df_coerce = pd.read_sql_query(sql='SELECT * FROM invoices', con=conn, coerce_float=True)\n",
    "    print(\"Example 7: Coercing floats (coerce_float=True):\")\n",
    "    print(df_coerce['Total'].dtype, '\\n')\n",
    "\n",
    "except Error as e:\n",
    "    print(f\"Error connecting to MySQL: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Close the connection\n",
    "    if 'conn' in locals() and conn.is_connected():\n",
    "        conn.close()\n",
    "        print(\"MySQL connection closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944fada0",
   "metadata": {},
   "source": [
    "# üìä Saving DataFrames to CSV Files in Pandas\n",
    "\n",
    "## üì§ Writing DataFrames to CSV with `to_csv()`\n",
    "\n",
    "The `pandas.DataFrame.to_csv()` method is used to save a Pandas DataFrame to a CSV (Comma-Separated Values) file, a widely used format for data storage and exchange. This method is flexible, allowing customization of delimiters, headers, encoding, and more. This guide explains the most useful and frequently used parameters of `to_csv()` with examples using a real-world dataset, focusing on practical applications for data export.\n",
    "\n",
    "### üîπ Dataset\n",
    "We‚Äôll use the **Iris Dataset**, which contains measurements of iris flowers, including columns like `sepal_length`, `sepal_width`, `petal_length`, `petal_width`, and `species`. The dataset is available as a tab-separated text file at:\n",
    "- URL: `https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.txt`\n",
    "- Process: We‚Äôll load the dataset into a DataFrame and save it to various CSV files to demonstrate `to_csv()` parameters.\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Most Useful `to_csv()` Parameters\n",
    "\n",
    "Below are the most useful and frequently used parameters for `pandas.DataFrame.to_csv()`, along with their descriptions and use cases.\n",
    "\n",
    "| Parameter          | Description                                              |\n",
    "|--------------------|----------------------------------------------------------|\n",
    "| `path_or_buf`     | File path or object where the CSV will be saved. If `None`, returns the CSV as a string. Required unless outputting to a buffer. |\n",
    "| `sep`             | Delimiter to use in the CSV file (e.g., `','`, `';'`, `'\\t'`). Default is `','`. Useful for non-standard delimiters. |\n",
    "| `index`           | If `True`, writes the DataFrame index to the CSV. Default is `True`. Set to `False` to exclude the index. |\n",
    "| `header`          | If `True`, writes column names as the first row. Default is `True`. Can also specify a list of custom column names. |\n",
    "| `columns`         | List of columns to write to the CSV. Useful for exporting a subset of the DataFrame‚Äôs columns. |\n",
    "| `na_rep`          | String to represent missing (NaN) values (e.g., `'N/A'`). Default is `''` (empty string). Enhances readability of missing data. |\n",
    "| `float_format`    | Format string for floating-point numbers (e.g., `'%.2f'` for two decimal places). Controls precision of numerical columns. |\n",
    "| `encoding`        | Encoding for the output file (e.g., `'utf-8'`, `'latin1'`). Default is `'utf-8'`. Handles special characters. |\n",
    "| `compression`     | Compression method for the output file (e.g., `'gzip'`, `'zip'`). Default is `'infer'` (based on file extension). Saves disk space for large files. |\n",
    "| `date_format`     | Format string for datetime columns (e.g., `'%Y-%m-%d'`). Useful for controlling date output format. |\n",
    "| `quotechar`       | Character used to quote fields (e.g., `'\"'`). Default is `'\"'`. Handles fields containing delimiters or special characters. |\n",
    "| `doublequote`     | If `True`, doubles the `quotechar` within quoted fields to escape it. Default is `True`. Ensures correct parsing of quoted fields. |\n",
    "| `line_terminator` | String to terminate lines (e.g., `'\\n'`). Default is system-specific (`'\\n'` on Unix, `'\\r\\n'` on Windows). Useful for cross-platform compatibility. |\n",
    "\n",
    "For a complete list of parameters, refer to the [official pandas documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_csv.html).\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Examples\n",
    "\n",
    "Below are examples of how to use each parameter with the Iris dataset, saving the DataFrame to various CSV files. These are implemented in a separate Python script (`to_csv_pandas_examples.py`).\n",
    "\n",
    "1. **Basic Usage (`path_or_buf`)**: Saves the DataFrame to a CSV file with default settings.\n",
    "2. **Specifying Separator (`sep`)**: Uses a custom delimiter (e.g., semicolon) instead of a comma.\n",
    "3. **Excluding Index (`index`)**: Saves the CSV without the DataFrame index.\n",
    "4. **Excluding Header (`header`)**: Saves the CSV without column names or with custom column names.\n",
    "5. **Selecting Columns (`columns`)**: Exports only specific columns to the CSV.\n",
    "6. **Handling Missing Values (`na_rep`)**: Represents missing values with a custom string (e.g., `'N/A'`).\n",
    "7. **Formatting Floats (`float_format`)**: Controls the precision of floating-point numbers.\n",
    "8. **Specifying Encoding (`encoding`)**: Uses a specific encoding (e.g., `'latin1'`) for the output file.\n",
    "9. **Applying Compression (`compression`)**: Saves the CSV as a compressed file (e.g., `.gz`).\n",
    "10. **Formatting Dates (`date_format`)**: Controls the format of datetime columns (demonstrated with a modified dataset).\n",
    "11. **Handling Quoted Fields (`quotechar`)**: Specifies the character used to quote fields.\n",
    "12. **Handling Double Quotes (`doublequote`)**: Escapes quote characters within fields.\n",
    "13. **Custom Line Terminator (`line_terminator`)**: Uses a specific line-ending character.\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Most Popular Parameters\n",
    "\n",
    "The following `to_csv()` parameters are the most commonly used in real-world data export tasks:\n",
    "- **`path_or_buf`**: Always required to specify the output file or buffer.\n",
    "- **`sep`**: Frequently used for non-comma delimiters (e.g., semicolon, tab) to meet specific requirements.\n",
    "- **`index`**: Often set to `False` to exclude the index, especially for datasets without meaningful indices.\n",
    "- **`header`**: Adjusted to exclude headers or use custom column names for specific formats.\n",
    "- **`na_rep`**: Widely used to represent missing values clearly (e.g., `'N/A'`, `'Missing'`).\n",
    "- **`float_format`**: Essential for controlling numerical precision in scientific or financial data.\n",
    "- **`encoding`**: Important for datasets with special characters or non-UTF-8 requirements.\n",
    "- **`compression`**: Useful for reducing file size when sharing or storing large datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Notes\n",
    "- **Dependencies**: Requires `pandas`. Install with:\n",
    "  ```bash\n",
    "  pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcd7bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### üìù Python Script: `to_csv_pandas_examples.py`\n",
    "\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "# Load the Iris dataset\n",
    "url = 'https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.txt'\n",
    "df = pd.read_csv(url, sep='\\t')\n",
    "\n",
    "# Add a datetime column for demonstrating date_format\n",
    "df['date_added'] = pd.date_range(start='2025-01-01', periods=len(df), freq='D')\n",
    "\n",
    "# Add some missing values for demonstrating na_rep\n",
    "df.loc[0:2, 'sepal_length'] = None\n",
    "\n",
    "# üîπ Example 1: Basic Usage (path_or_buf)\n",
    "df.to_csv('iris_basic.csv')\n",
    "print(\"Example 1: Saved to 'iris_basic.csv' with default settings\\n\")\n",
    "\n",
    "# üîπ Example 2: Specifying separator (sep)\n",
    "df.to_csv('iris_semicolon.csv', sep=';')\n",
    "print(\"Example 2: Saved to 'iris_semicolon.csv' with semicolon separator\\n\")\n",
    "\n",
    "# üîπ Example 3: Excluding index (index)\n",
    "df.to_csv('iris_no_index.csv', index=False)\n",
    "print(\"Example 3: Saved to 'iris_no_index.csv' without index\\n\")\n",
    "\n",
    "# üîπ Example 4: Excluding header (header)\n",
    "df.to_csv('iris_no_header.csv', header=False)\n",
    "print(\"Example 4: Saved to 'iris_no_header.csv' without header\\n\")\n",
    "# With custom header\n",
    "df.to_csv('iris_custom_header.csv', header=['SL', 'SW', 'PL', 'PW', 'Type', 'Date'])\n",
    "print(\"Example 4b: Saved to 'iris_custom_header.csv' with custom header\\n\")\n",
    "\n",
    "# üîπ Example 5: Selecting specific columns (columns)\n",
    "df.to_csv('iris_subset.csv', columns=['sepal_length', 'sepal_width', 'species'])\n",
    "print(\"Example 5: Saved to 'iris_subset.csv' with selected columns\\n\")\n",
    "\n",
    "# üîπ Example 6: Handling missing values (na_rep)\n",
    "df.to_csv('iris_na.csv', na_rep='N/A')\n",
    "print(\"Example 6: Saved to 'iris_na.csv' with missing values as 'N/A'\\n\")\n",
    "\n",
    "# üîπ Example 7: Formatting floats (float_format)\n",
    "df.to_csv('iris_float.csv', float_format='%.2f')\n",
    "print(\"Example 7: Saved to 'iris_float.csv' with two decimal places\\n\")\n",
    "\n",
    "# üîπ Example 8: Specifying encoding (encoding)\n",
    "df.to_csv('iris_latin1.csv', encoding='latin1')\n",
    "print(\"Example 8: Saved to 'iris_latin1.csv' with latin1 encoding\\n\")\n",
    "\n",
    "# üîπ Example 9: Applying compression (compression)\n",
    "df.to_csv('iris_compressed.csv.gz', compression='gzip')\n",
    "print(\"Example 9: Saved to 'iris_compressed.csv.gz' with gzip compression\\n\")\n",
    "\n",
    "# üîπ Example 10: Formatting dates (date_format)\n",
    "df.to_csv('iris_dates.csv', date_format='%Y-%m-%d')\n",
    "print(\"Example 10: Saved to 'iris_dates.csv' with custom date format\\n\")\n",
    "\n",
    "# üîπ Example 11: Handling quoted fields (quotechar)\n",
    "df.to_csv('iris_quoted.csv', quotechar=\"'\")\n",
    "print(\"Example 11: Saved to 'iris_quoted.csv' with single-quote quoting\\n\")\n",
    "\n",
    "# üîπ Example 12: Handling double quotes (doublequote)\n",
    "df.to_csv('iris_doublequote.csv', doublequote=True)\n",
    "print(\"Example 12: Saved to 'iris_doublequote.csv' with doublequote escaping\\n\")\n",
    "\n",
    "# üîπ Example 13: Custom line terminator (line_terminator)\n",
    "df.to_csv('iris_lineterm.csv', line_terminator='\\r\\n')\n",
    "print(\"Example 13: Saved to 'iris_lineterm.csv' with custom line terminator\\n\")\n",
    "\n",
    "# Verify one of the outputs\n",
    "print(\"Sample content of 'iris_basic.csv':\")\n",
    "with open('iris_basic.csv', 'r') as f:\n",
    "    print(f.read()[:200], '...\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f2d35b",
   "metadata": {},
   "source": [
    "# üìä Saving DataFrames to Excel Files in Pandas\n",
    "\n",
    "## üì§ Writing DataFrames to Excel with `to_excel()`\n",
    "\n",
    "The `pandas.DataFrame.to_excel()` method is used to save a Pandas DataFrame to an Excel file (`.xlsx` or `.xls`), a popular format for data analysis, reporting, and sharing in business environments. This method leverages Excel writer engines like `openpyxl` or `xlsxwriter` for flexible output customization. This guide explains the most useful and frequently used parameters of `to_excel()` with examples using a real-world dataset, focusing on practical applications for data export.\n",
    "\n",
    "### üîπ Dataset\n",
    "We‚Äôll use the **Iris Dataset**, which contains measurements of iris flowers, including columns like `sepal_length`, `sepal_width`, `petal_length`, `petal_width`, and `species`. The dataset is available as a tab-separated text file at:\n",
    "- URL: `https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.txt`\n",
    "- Process: We‚Äôll load the dataset into a DataFrame and save it to various Excel files to demonstrate `to_excel()` parameters.\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Most Useful `to_excel()` Parameters\n",
    "\n",
    "Below are the most useful and frequently used parameters for `pandas.DataFrame.to_excel()`, along with their descriptions and use cases.\n",
    "\n",
    "| Parameter          | Description                                              |\n",
    "|--------------------|----------------------------------------------------------|\n",
    "| `excel_writer`    | File path or `ExcelWriter` object where the Excel file will be saved. Required to specify the output. |\n",
    "| `sheet_name`      | Name of the sheet to write the DataFrame to. Default is `'Sheet1'`. Useful for multi-sheet Excel files. |\n",
    "| `index`           | If `True`, writes the DataFrame index to the Excel file. Default is `True`. Set to `False` to exclude the index. |\n",
    "| `header`          | If `True`, writes column names as the first row. Default is `True`. Can specify a list of custom column names. |\n",
    "| `columns`         | List of columns to write to the Excel file. Useful for exporting a subset of the DataFrame‚Äôs columns. |\n",
    "| `na_rep`          | String to represent missing (NaN) values (e.g., `'N/A'`). Default is `''` (empty string). Enhances readability of missing data. |\n",
    "| `float_format`    | Format string for floating-point numbers (e.g., `'%.2f'` for two decimal places). Controls precision of numerical columns. |\n",
    "| `startrow`        | Upper-left row index (0-based) to start writing the DataFrame. Default is `0`. Useful for adding headers or metadata above the data. |\n",
    "| `startcol`        | Upper-left column index (0-based) to start writing the DataFrame. Default is `0`. Useful for offsetting data in the sheet. |\n",
    "| `engine`          | Excel writer engine to use (`'openpyxl'`, `'xlsxwriter'`, or `'xlwt'`). Default is `None` (auto-detect). Specify for compatibility or performance. |\n",
    "| `date_format`     | Format string for datetime columns (e.g., `'YYYY-MM-DD'`). Useful for controlling date output format. |\n",
    "| `encoding`        | Encoding for the output file (e.g., `'utf-8'`). Deprecated in recent versions; included for legacy compatibility with `'xlwt'`. |\n",
    "\n",
    "For a complete list of parameters, refer to the [official pandas documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_excel.html).\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Examples\n",
    "\n",
    "Below are examples of how to use each parameter with the Iris dataset, saving the DataFrame to various Excel files. These are implemented in a separate Python script (`to_excel_pandas_examples.py`).\n",
    "\n",
    "1. **Basic Usage (`excel_writer`)**: Saves the DataFrame to an Excel file with default settings.\n",
    "2. **Specifying Sheet Name (`sheet_name`)**: Writes the DataFrame to a named sheet.\n",
    "3. **Excluding Index (`index`)**: Saves the Excel file without the DataFrame index.\n",
    "4. **Excluding Header (`header`)**: Saves the Excel file without column names or with custom column names.\n",
    "5. **Selecting Columns (`columns`)**: Exports only specific columns to the Excel file.\n",
    "6. **Handling Missing Values (`na_rep`)**: Represents missing values with a custom string (e.g., `'N/A'`).\n",
    "7. **Formatting Floats (`float_format`)**: Controls the precision of floating-point numbers.\n",
    "8. **Offsetting Data (`startrow`, `startcol`)**: Writes the DataFrame starting at a specific row and column.\n",
    "9. **Specifying Engine (`engine`)**: Uses a specific Excel writer engine (e.g., `openpyxl`).\n",
    "10. **Formatting Dates (`date_format`)**: Controls the format of datetime columns (demonstrated with a modified dataset).\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Most Popular Parameters\n",
    "\n",
    "The following `to_excel()` parameters are the most commonly used in real-world data export tasks:\n",
    "- **`excel_writer`**: Always required to specify the output file or writer object.\n",
    "- **`sheet_name`**: Frequently used to organize data in multi-sheet Excel files.\n",
    "- **`index`**: Often set to `False` to exclude the index, especially for datasets without meaningful indices.\n",
    "- **`header`**: Adjusted to exclude headers or use custom column names for reporting purposes.\n",
    "- **`na_rep`**: Widely used to represent missing values clearly (e.g., `'N/A'`, `'Missing'`).\n",
    "- **`float_format`**: Essential for controlling numerical precision in financial or scientific data.\n",
    "- **`startrow` and `startcol`**: Useful for formatting Excel files with headers, titles, or multiple datasets.\n",
    "- **`engine`**: Often specified to ensure compatibility or leverage specific features (e.g., `openpyxl` for modern `.xlsx` files).\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Notes\n",
    "- **Dependencies**: Requires `pandas` and an Excel writer engine (`openpyxl` for `.xlsx` files or `xlsxwriter` for advanced formatting). Install with:\n",
    "  ```bash\n",
    "  pip install pandas openpyxl xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901e908e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### üìù Python Script: `to_excel_pandas_examples.py`\n",
    "\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "# Load the Iris dataset\n",
    "url = 'https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.txt'\n",
    "df = pd.read_csv(url, sep='\\t')\n",
    "\n",
    "# Add a datetime column for demonstrating date_format\n",
    "df['date_added'] = pd.date_range(start='2025-01-01', periods=len(df), freq='D')\n",
    "\n",
    "# Add some missing values for demonstrating na_rep\n",
    "df.loc[0:2, 'sepal_length'] = None\n",
    "\n",
    "# üîπ Example 1: Basic Usage (excel_writer)\n",
    "df.to_excel('iris_basic.xlsx')\n",
    "print(\"Example 1: Saved to 'iris_basic.xlsx' with default settings\\n\")\n",
    "\n",
    "# üîπ Example 2: Specifying sheet name (sheet_name)\n",
    "df.to_excel('iris_sheet.xlsx', sheet_name='IrisData')\n",
    "print(\"Example 2: Saved to 'iris_sheet.xlsx' with sheet name 'IrisData'\\n\")\n",
    "\n",
    "# üîπ Example 3: Excluding index (index)\n",
    "df.to_excel('iris_no_index.xlsx', index=False)\n",
    "print(\"Example 3: Saved to 'iris_no_index.xlsx' without index\\n\")\n",
    "\n",
    "# üîπ Example 4: Excluding header (header)\n",
    "df.to_excel('iris_no_header.xlsx', header=False)\n",
    "print(\"Example 4: Saved to 'iris_no_header.xlsx' without header\\n\")\n",
    "# With custom header\n",
    "df.to_excel('iris_custom_header.xlsx', header=['SL', 'SW', 'PL', 'PW', 'Type', 'Date'])\n",
    "print(\"Example 4b: Saved to 'iris_custom_header.xlsx' with custom header\\n\")\n",
    "\n",
    "# üîπ Example 5: Selecting specific columns (columns)\n",
    "df.to_excel('iris_subset.xlsx', columns=['sepal_length', 'sepal_width', 'species'])\n",
    "print(\"Example 5: Saved to 'iris_subset.xlsx' with selected columns\\n\")\n",
    "\n",
    "# üîπ Example 6: Handling missing values (na_rep)\n",
    "df.to_excel('iris_na.xlsx', na_rep='N/A')\n",
    "print(\"Example 6: Saved to 'iris_na.xlsx' with missing values as 'N/A'\\n\")\n",
    "\n",
    "# üîπ Example 7: Formatting floats (float_format)\n",
    "df.to_excel('iris_float.xlsx', float_format='%.2f')\n",
    "print(\"Example 7: Saved to 'iris_float.xlsx' with two decimal places\\n\")\n",
    "\n",
    "# üîπ Example 8: Offsetting data (startrow, startcol)\n",
    "df.to_excel('iris_offset.xlsx', startrow=2, startcol=1)\n",
    "print(\"Example 8: Saved to 'iris_offset.xlsx' with offset (startrow=2, startcol=1)\\n\")\n",
    "\n",
    "# üîπ Example 9: Specifying engine (engine)\n",
    "df.to_excel('iris_openpyxl.xlsx', engine='openpyxl')\n",
    "print(\"Example 9: Saved to 'iris_openpyxl.xlsx' with openpyxl engine\\n\")\n",
    "\n",
    "# üîπ Example 10: Formatting dates (date_format)\n",
    "df.to_excel('iris_dates.xlsx', date_format='%Y-%m-%d')\n",
    "print(\"Example 10: Saved to 'iris_dates.xlsx' with custom date format\\n\")\n",
    "\n",
    "# Verify one of the outputs by reading it back\n",
    "print(\"Sample content of 'iris_basic.xlsx':\")\n",
    "df_read = pd.read_excel('iris_basic.xlsx')\n",
    "print(df_read.head(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859bbe24",
   "metadata": {},
   "source": [
    "# üìä Saving DataFrames to Excel Files in Pandas\n",
    "\n",
    "## üì§ Writing DataFrames to Excel with `to_excel()` and `ExcelWriter`\n",
    "\n",
    "The `pandas.DataFrame.to_excel()` method is used to save a Pandas DataFrame to an Excel file (`.xlsx` or `.xls`), a popular format for data analysis, reporting, and sharing. When combined with `pandas.ExcelWriter`, it allows writing multiple DataFrames to different sheets in a single Excel file, providing greater flexibility for complex outputs. This guide explains the most useful parameters of `to_excel()`, including how to use `ExcelWriter` for multi-sheet exports, with examples using a real-world dataset.\n",
    "\n",
    "### üîπ Dataset\n",
    "We‚Äôll use the **Iris Dataset**, which contains measurements of iris flowers, including columns like `sepal_length`, `sepal_width`, `petal_length`, `petal_width`, and `species`. The dataset is available as a tab-separated text file at:\n",
    "- URL: `https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.txt`\n",
    "- Process: We‚Äôll load the dataset into a DataFrame, split it into two DataFrames (`df1` and `df2`), and save them to Excel files to demonstrate `to_excel()` and `ExcelWriter`.\n",
    "\n",
    "### üîπ Using `ExcelWriter`\n",
    "The `pandas.ExcelWriter` class is a context manager that allows writing multiple DataFrames to different sheets in a single Excel file. It supports engines like `openpyxl` or `xlsxwriter` and is used with a `with` statement for proper resource management.\n",
    "\n",
    "Example usage:\n",
    "```python\n",
    "with pd.ExcelWriter('output.xlsx') as writer:\n",
    "    df1.to_excel(writer, sheet_name='Sheet1')\n",
    "    df2.to_excel(writer, sheet_name='Sheet2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14de53b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### üìù Python Script: `to_excel_pandas_examples.py`\n",
    "\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "# Load the Iris dataset\n",
    "url = 'https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.txt'\n",
    "df = pd.read_csv(url, sep='\\t')\n",
    "\n",
    "# Add a datetime column for demonstrating date_format\n",
    "df['date_added'] = pd.date_range(start='2025-01-01', periods=len(df), freq='D')\n",
    "\n",
    "# Add some missing values for demonstrating na_rep\n",
    "df.loc[0:2, 'sepal_length'] = None\n",
    "\n",
    "# Split the DataFrame into two for multi-sheet demonstration\n",
    "df1 = df.iloc[:75]  # First 75 rows\n",
    "df2 = df.iloc[75:]  # Remaining rows\n",
    "\n",
    "# üîπ Example 1: Basic Usage (excel_writer)\n",
    "df.to_excel('iris_basic.xlsx')\n",
    "print(\"Example 1: Saved to 'iris_basic.xlsx' with default settings\\n\")\n",
    "\n",
    "# üîπ Example 2: Using ExcelWriter for multiple sheets\n",
    "with pd.ExcelWriter('iris_multi_sheet.xlsx') as writer:\n",
    "    df1.to_excel(writer, sheet_name='Sheet1')\n",
    "    df2.to_excel(writer, sheet_name='Sheet2')\n",
    "print(\"Example 2: Saved df1 to 'Sheet1' and df2 to 'Sheet2' in 'iris_multi_sheet.xlsx'\\n\")\n",
    "\n",
    "# üîπ Example 3: Specifying sheet name (sheet_name)\n",
    "df.to_excel('iris_sheet.xlsx', sheet_name='IrisData')\n",
    "print(\"Example 3: Saved to 'iris_sheet.xlsx' with sheet name 'IrisData'\\n\")\n",
    "\n",
    "# üîπ Example 4: Excluding index (index)\n",
    "df.to_excel('iris_no_index.xlsx', index=False)\n",
    "print(\"Example 4: Saved to 'iris_no_index.xlsx' without index\\n\")\n",
    "\n",
    "# üîπ Example 5: Excluding header (header)\n",
    "df.to_excel('iris_no_header.xlsx', header=False)\n",
    "print(\"Example 5: Saved to 'iris_no_header.xlsx' without header\\n\")\n",
    "# With custom header\n",
    "df.to_excel('iris_custom_header.xlsx', header=['SL', 'SW', 'PL', 'PW', 'Type', 'Date'])\n",
    "print(\"Example 5b: Saved to 'iris_custom_header.xlsx' with custom header\\n\")\n",
    "\n",
    "# üîπ Example 6: Selecting specific columns (columns)\n",
    "df.to_excel('iris_subset.xlsx', columns=['sepal_length', 'sepal_width', 'species'])\n",
    "print(\"Example 6: Saved to 'iris_subset.xlsx' with selected columns\\n\")\n",
    "\n",
    "# üîπ Example 7: Handling missing values (na_rep)\n",
    "df.to_excel('iris_na.xlsx', na_rep='N/A')\n",
    "print(\"Example 7: Saved to 'iris_na.xlsx' with missing values as 'N/A'\\n\")\n",
    "\n",
    "# üîπ Example 8: Formatting floats (float_format)\n",
    "df.to_excel('iris_float.xlsx', float_format='%.2f')\n",
    "print(\"Example 8: Saved to 'iris_float.xlsx' with two decimal places\\n\")\n",
    "\n",
    "# üîπ Example 9: Offsetting data (startrow, startcol)\n",
    "df.to_excel('iris_offset.xlsx', startrow=2, startcol=1)\n",
    "print(\"Example 9: Saved to 'iris_offset.xlsx' with offset (startrow=2, startcol=1)\\n\")\n",
    "\n",
    "# üîπ Example 10: Specifying engine (engine)\n",
    "with pd.ExcelWriter('iris_openpyxl.xlsx', engine='openpyxl') as writer:\n",
    "    df.to_excel(writer, sheet_name='Data')\n",
    "print(\"Example 10: Saved to 'iris_openpyxl.xlsx' with openpyxl engine\\n\")\n",
    "\n",
    "# üîπ Example 11: Formatting dates (date_format)\n",
    "df.to_excel('iris_dates.xlsx', date_format='%Y-%m-%d')\n",
    "print(\"Example 11: Saved to 'iris_dates.xlsx' with custom date format\\n\")\n",
    "\n",
    "# Verify one of the outputs by reading it back\n",
    "print(\"Sample content of 'iris_multi_sheet.xlsx' (Sheet1):\")\n",
    "df_read = pd.read_excel('iris_multi_sheet.xlsx', sheet_name='Sheet1')\n",
    "print(df_read.head(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a12398a",
   "metadata": {},
   "source": [
    "# üìä Saving DataFrames to SQL Databases in Pandas\n",
    "\n",
    "## üì§ Writing DataFrames to SQL with `to_sql()`\n",
    "\n",
    "The `pandas.DataFrame.to_sql()` method is used to save a Pandas DataFrame to a SQL database table, enabling seamless integration with relational databases like MySQL, PostgreSQL, or SQLite. When paired with **MySQL Connector/Python**, it allows direct interaction with a MySQL database managed locally using **XAMPP**. This guide explains the most useful and frequently used parameters of `to_sql()` with examples using a real-world dataset, focusing on practical applications for data export to a MySQL database.\n",
    "\n",
    "### üîπ Dataset\n",
    "We‚Äôll use the **Iris Dataset**, which contains measurements of iris flowers, including columns like `sepal_length`, `sepal_width`, `petal_length`, `petal_width`, and `species`. The dataset is available as a tab-separated text file at:\n",
    "- URL: `https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.txt`\n",
    "- Process: We‚Äôll load the dataset into a DataFrame and save it to a MySQL table named `iris` in a database named `test_db`.\n",
    "\n",
    "### üîπ Setting Up MySQL with XAMPP\n",
    "\n",
    "**XAMPP** is a free, open-source software stack that includes Apache, MySQL, PHP, and phpMyAdmin, making it easy to set up a local MySQL server for development.\n",
    "\n",
    "#### Installation Steps\n",
    "1. **Download XAMPP**:\n",
    "   - Visit [Apache Friends](https://www.apachefriends.org/index.html) and download XAMPP for your operating system (Windows, macOS, or Linux).\n",
    "   - Verified as accessible on June 4, 2025, at 3:21 PM IST.\n",
    "2. **Install XAMPP**:\n",
    "   - Run the installer and select components: Apache, MySQL, and phpMyAdmin.\n",
    "   - Install to a default location (e.g., `C:\\xampp` on Windows).\n",
    "3. **Start MySQL**:\n",
    "   - Open the XAMPP Control Panel.\n",
    "   - Start the **Apache** and **MySQL** modules.\n",
    "   - MySQL typically runs on port `3306` (default).\n",
    "4. **Access phpMyAdmin**:\n",
    "   - Open a browser and navigate to `http://localhost/phpmyadmin`.\n",
    "   - Log in with the default credentials (username: `root`, password: usually empty unless changed).\n",
    "5. **Create the Database**:\n",
    "   - In phpMyAdmin, click **New** to create a database named `test_db`.\n",
    "   - No tables are needed initially, as `to_sql()` will create the `iris` table automatically.\n",
    "\n",
    "#### XAMPP Configuration Notes\n",
    "- **Default Credentials**: Username is `root`, password is empty by default. Set a password for security in production environments.\n",
    "- **Port Conflicts**: Ensure port `3306` is free. Check the XAMPP Control Panel or MySQL logs if MySQL fails to start.\n",
    "- **Accessing MySQL**: Use `localhost` or `127.0.0.1` as the host for local connections.\n",
    "- **Security**: For production, configure user permissions and enable a firewall to restrict database access.\n",
    "\n",
    "### üîπ Connecting to MySQL with MySQL Connector/Python\n",
    "\n",
    "**MySQL Connector/Python** is a pure Python driver for MySQL, allowing Python applications to connect to MySQL databases.\n",
    "\n",
    "#### Installation\n",
    "Install the connector using pip:\n",
    "```bash\n",
    "pip install mysql-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87a07e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### üìù Python Script: `to_sql_pandas_examples.py`\n",
    "\n",
    "import pandas as pd\n",
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "\n",
    "# Load the Iris dataset\n",
    "url = 'https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.txt'\n",
    "df = pd.read_csv(url, sep='\\t')\n",
    "\n",
    "# Add some missing values for demonstration\n",
    "df.loc[0:2, 'sepal_length'] = None\n",
    "\n",
    "# MySQL connection configuration (update as needed)\n",
    "config = {\n",
    "    'host': 'localhost',\n",
    "    'user': 'root',\n",
    "    'password': '',  # Update if you set a password in XAMPP\n",
    "    'database': 'test_db'\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Establish MySQL connection\n",
    "    conn = mysql.connector.connect(**config)\n",
    "\n",
    "    # üîπ Example 1: Basic Usage (name, con)\n",
    "    df.to_sql(name='iris', con=conn, if_exists='replace')\n",
    "    print(\"Example 1: Saved DataFrame to 'iris' table\\n\")\n",
    "    # Verify by reading back\n",
    "    df_read = pd.read_sql_query('SELECT * FROM iris LIMIT 5', conn)\n",
    "    print(df_read, '\\n')\n",
    "\n",
    "    # üîπ Example 2: Handling existing tables (if_exists)\n",
    "    # Append a subset of the DataFrame to the same table\n",
    "    df_subset = df.head(10)\n",
    "    df_subset.to_sql(name='iris', con=conn, if_exists='append')\n",
    "    print(\"Example 2: Appended 10 rows to 'iris' table (if_exists='append')\\n\")\n",
    "    # Verify row count\n",
    "    count = pd.read_sql_query('SELECT COUNT(*) AS count FROM iris', conn)['count'].iloc[0]\n",
    "    print(f\"Total rows in 'iris' table: {count}\\n\")\n",
    "\n",
    "    # üîπ Example 3: Excluding index (index)\n",
    "    df.to_sql(name='iris_no_index', con=conn, if_exists='replace', index=False)\n",
    "    print(\"Example 3: Saved to 'iris_no_index' table without index\\n\")\n",
    "    # Verify columns\n",
    "    df_read_no_index = pd.read_sql_query('SELECT * FROM iris_no_index LIMIT 5', conn)\n",
    "    print(df_read_no_index, '\\n')\n",
    "\n",
    "    # üîπ Example 4: Custom index label (index_label)\n",
    "    df.to_sql(name='iris_custom_index', con=conn, if_exists='replace', index=True, index_label='record_id')\n",
    "    print(\"Example 4: Saved to 'iris_custom_index' table with index labeled 'record_id'\\n\")\n",
    "    # Verify index column\n",
    "    df_read_custom_index = pd.read_sql_query('SELECT record_id, sepal_length FROM iris_custom_index LIMIT 5', conn)\n",
    "    print(df_read_custom_index, '\\n')\n",
    "\n",
    "    # üîπ Example 5: Specifying data types (dtype)\n",
    "    dtype_dict = {\n",
    "        'sepal_length': 'FLOAT',\n",
    "        'sepal_width': 'FLOAT',\n",
    "        'petal_length': 'FLOAT',\n",
    "        'petal_width': 'FLOAT',\n",
    "        'species': 'VARCHAR(50)'\n",
    "    }\n",
    "    df.to_sql(name='iris_dtype', con=conn, if_exists='replace', dtype=dtype_dict)\n",
    "    print(\"Example 5: Saved to 'iris_dtype' table with custom SQL data types\\n\")\n",
    "    # Verify schema\n",
    "    schema = pd.read_sql_query(\"SHOW COLUMNS FROM iris_dtype\", conn)\n",
    "    print(schema, '\\n')\n",
    "\n",
    "    # üîπ Example 6: Writing in chunks (chunksize)\n",
    "    df.to_sql(name='iris_chunks', con=conn, if_exists='replace', chunksize=50)\n",
    "    print(\"Example 6: Saved to 'iris_chunks' table in chunks of 50 rows\\n\")\n",
    "    # Verify\n",
    "    df_read_chunks = pd.read_sql_query('SELECT * FROM iris_chunks LIMIT 5', conn)\n",
    "    print(df_read_chunks, '\\n')\n",
    "\n",
    "    # üîπ Example 7: Using multi-row insert (method='multi')\n",
    "    df.to_sql(name='iris_multi', con=conn, if_exists='replace', method='multi')\n",
    "    print(\"Example 7: Saved to 'iris_multi' table using multi-row insert\\n\")\n",
    "    # Verify\n",
    "    df_read_multi = pd.read_sql_query('SELECT * FROM iris_multi LIMIT 5', conn)\n",
    "    print(df_read_multi, '\\n')\n",
    "\n",
    "except Error as e:\n",
    "    print(f\"Error connecting to MySQL: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Close the connection\n",
    "    if 'conn' in locals() and conn.is_connected():\n",
    "        conn.close()\n",
    "        print(\"MySQL connection closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4750f336",
   "metadata": {},
   "source": [
    "# üìä Converting DataFrames to HTML in Pandas\n",
    "\n",
    "## üì§ Writing DataFrames to HTML with `to_html()`\n",
    "\n",
    "The `pandas.DataFrame.to_html()` method converts a Pandas DataFrame to an HTML table, generating a string or file containing HTML `<table>` markup. This is ideal for displaying data in web applications, embedding in web pages, or sharing in HTML-based reports. The method offers customization options for table formatting, styling, and content. This guide explains the most useful and frequently used parameters of `to_html()` with examples using a real-world dataset, focusing on practical applications for HTML export.\n",
    "\n",
    "### üîπ Dataset\n",
    "We‚Äôll use the **Iris Dataset**, which contains measurements of iris flowers, including columns like `sepal_length`, `sepal_width`, `petal_length`, `petal_width`, and `species`. The dataset is available as a tab-separated text file at:\n",
    "- URL: `https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.txt`\n",
    "- Process: We‚Äôll load the dataset into a DataFrame and convert it to HTML tables to demonstrate `to_html()` parameters.\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Most Useful `to_html()` Parameters\n",
    "\n",
    "Below are the most useful and frequently used parameters for `pandas.DataFrame.to_html()`, along with their descriptions and use cases.\n",
    "\n",
    "| Parameter          | Description                                              |\n",
    "|--------------------|----------------------------------------------------------|\n",
    "| `buf`             | File path or file-like object to write the HTML output. If `None`, returns the HTML as a string. Default is `None`. |\n",
    "| `columns`         | List of columns to include in the HTML table. Useful for exporting a subset of the DataFrame‚Äôs columns. |\n",
    "| `index`           | If `True`, includes the DataFrame index in the HTML table. Default is `True`. Set to `False` to exclude the index. |\n",
    "| `header`          | If `True`, includes column names in the HTML table header. Default is `True`. Can specify a list of custom column names. |\n",
    "| `na_rep`          | String to represent missing (NaN) values (e.g., `'N/A'`). Default is `'NaN'`. Enhances readability of missing data. |\n",
    "| `float_format`    | Format string or callable for floating-point numbers (e.g., `'%.2f'`). Controls precision of numerical columns. |\n",
    "| `formatters`      | Dictionary or list of functions to format specific columns or all values. Useful for custom string representations (e.g., currency, percentages). |\n",
    "| `classes`         | CSS class(es) to apply to the `<table>` element (e.g., `'table table-striped'`). Supports string or list of strings for styling with CSS frameworks like Bootstrap. |\n",
    "| `border`          | Integer for the table border width (e.g., `1` for a visible border). Default is `None` (uses browser default). Set to `0` for no border. |\n",
    "| `index_names`     | If `True`, includes the index name(s) in the HTML table. Default is `True`. Set to `False` to exclude index names. |\n",
    "| `justify`         | Text alignment for table headers (`'left'`, `'right'`, `'center'`, etc.). Default is `'center'`. Controls header alignment. |\n",
    "| `escape`          | If `True`, escapes HTML characters (e.g., `<` becomes `&lt;`). Default is `True`. Set to `False` to allow raw HTML in the table. |\n",
    "| `render_links`    | If `True`, converts URLs in the DataFrame to clickable `<a>` tags. Default is `False`. Useful for URL columns. |\n",
    "\n",
    "For a complete list of parameters, refer to the [official pandas documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_html.html).\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Examples\n",
    "\n",
    "Below are examples of how to use each parameter with the Iris dataset, converting the DataFrame to HTML tables and saving them to files or displaying the output. These are implemented in a separate Python script (`to_html_pandas_examples.py`).\n",
    "\n",
    "1. **Basic Usage (`buf`)**: Converts the DataFrame to an HTML table and saves it to a file or returns as a string.\n",
    "2. **Selecting Columns (`columns`)**: Includes only specific columns in the HTML table.\n",
    "3. **Excluding Index (`index`)**: Generates the HTML table without the DataFrame index.\n",
    "4. **Excluding Header (`header`)**: Generates the HTML table without column names or with custom names.\n",
    "5. **Handling Missing Values (`na_rep`)**: Represents missing values with a custom string (e.g., `'N/A'`).\n",
    "6. **Formatting Floats (`float_format`)**: Controls the precision of floating-point numbers.\n",
    "7. **Custom Formatters (`formatters`)**: Applies custom formatting to specific columns (e.g., adding units).\n",
    "8. **Adding CSS Classes (`classes`)**: Applies CSS classes for styling (e.g., Bootstrap styles).\n",
    "9. **Setting Border (`border`)**: Specifies the table border width.\n",
    "10. **Excluding Index Names (`index_names`)**: Generates the HTML table without index names.\n",
    "11. **Justifying Headers (`justify`)**: Aligns table headers (e.g., left-aligned).\n",
    "12. **Disabling HTML Escaping (`escape`)**: Allows raw HTML in the table content.\n",
    "13. **Rendering Links (`render_links`)**: Converts URLs to clickable links (demonstrated with a modified dataset).\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Most Popular Parameters\n",
    "\n",
    "The following `to_html()` parameters are the most commonly used in real-world HTML export tasks:\n",
    "- **`buf`**: Frequently used to save the HTML output to a file or capture it as a string for web integration.\n",
    "- **`columns`**: Useful for exporting only relevant columns to keep the table concise.\n",
    "- **`index`**: Often set to `False` to exclude the index, especially for datasets without meaningful indices.\n",
    "- **`header`**: Adjusted to exclude headers or use custom column names for display purposes.\n",
    "- **`na_rep`**: Widely used to represent missing values clearly (e.g., `'N/A'`, `'Missing'`).\n",
    "- **`classes`**: Essential for applying CSS styles, especially with frameworks like Bootstrap or custom CSS.\n",
    "- **`float_format`**: Controls numerical precision for readability in web displays.\n",
    "- **`escape`**: Set to `False` when embedding raw HTML or JavaScript in the table.\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Notes\n",
    "- **Dependencies**: Requires `pandas`. Install with:\n",
    "  ```bash\n",
    "  pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2c7ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### üìù Python Script: `to_html_pandas_examples.py`\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Iris dataset\n",
    "url = 'https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.txt'\n",
    "df = pd.read_csv(url, sep='\\t')\n",
    "\n",
    "# Add some missing values for demonstrating na_rep\n",
    "df.loc[0:2, 'sepal_length'] = None\n",
    "\n",
    "# Add a URL column for demonstrating render_links\n",
    "df['url'] = [f'https://example.com/iris/{i}' for i in range(len(df))]\n",
    "\n",
    "# Add a column with HTML content for demonstrating escape\n",
    "df['description'] = [f'<b>Iris {i}</b>' for i in range(len(df))]\n",
    "\n",
    "# üîπ Example 1: Basic Usage (buf)\n",
    "# Save to file\n",
    "df.to_html('iris_basic.html')\n",
    "print(\"Example 1a: Saved to 'iris_basic.html' with default settings\")\n",
    "# Return as string\n",
    "html_str = df.to_html()\n",
    "print(\"Example 1b: HTML string (first 200 characters):\")\n",
    "print(html_str[:200], '...\\n')\n",
    "\n",
    "# üîπ Example 2: Selecting specific columns (columns)\n",
    "df.to_html('iris_subset.html', columns=['sepal_length', 'sepal_width', 'species'])\n",
    "print(\"Example 2: Saved to 'iris_subset.html' with selected columns\\n\")\n",
    "\n",
    "# üîπ Example 3: Excluding index (index)\n",
    "df.to_html('iris_no_index.html', index=False)\n",
    "print(\"Example 3: Saved to 'iris_no_index.html' without index\\n\")\n",
    "\n",
    "# üîπ Example 4: Excluding header (header)\n",
    "df.to_html('iris_no_header.html', header=False)\n",
    "print(\"Example 4: Saved to 'iris_no_header.html' without header\\n\")\n",
    "# With custom header\n",
    "df.to_html('iris_custom_header.html', header=['SL', 'SW', 'PL', 'PW', 'Type', 'URL', 'Desc'])\n",
    "print(\"Example 4b: Saved to 'iris_custom_header.html' with custom header\\n\")\n",
    "\n",
    "# üîπ Example 5: Handling missing values (na_rep)\n",
    "df.to_html('iris_na.html', na_rep='N/A')\n",
    "print(\"Example 5: Saved to 'iris_na.html' with missing values as 'N/A'\\n\")\n",
    "\n",
    "# üîπ Example 6: Formatting floats (float_format)\n",
    "df.to_html('iris_float.html', float_format='%.2f')\n",
    "print(\"Example 6: Saved to 'iris_float.html' with two decimal places\\n\")\n",
    "\n",
    "# üîπ Example 7: Custom formatters (formatters)\n",
    "formatters = {\n",
    "    'sepal_length': lambda x: f'{x:.1f} cm' if pd.notnull(x) else 'N/A',\n",
    "    'species': lambda x: x.upper()\n",
    "}\n",
    "df.to_html('iris_formatters.html', formatters=formatters)\n",
    "print(\"Example 7: Saved to 'iris_formatters.html' with custom formatters\\n\")\n",
    "\n",
    "# üîπ Example 8: Adding CSS classes (classes)\n",
    "df.to_html('iris_styled.html', classes=['table', 'table-striped'])\n",
    "print(\"Example 8: Saved to 'iris_styled.html' with Bootstrap CSS classes\\n\")\n",
    "\n",
    "# üîπ Example 9: Setting border (border)\n",
    "df.to_html('iris_border.html', border=1)\n",
    "print(\"Example 9: Saved to 'iris_border.html' with border width 1\\n\")\n",
    "\n",
    "# üîπ Example 10: Excluding index names (index_names)\n",
    "df.to_html('iris_no_index_names.html', index_names=False)\n",
    "print(\"Example 10: Saved to 'iris_no_index_names.html' without index names\\n\")\n",
    "\n",
    "# üîπ Example 11: Justifying headers (justify)\n",
    "df.to_html('iris_justify.html', justify='left')\n",
    "print(\"Example 11: Saved to 'iris_justify.html' with left-justified headers\\n\")\n",
    "\n",
    "# üîπ Example 12: Disabling HTML escaping (escape)\n",
    "df.to_html('iris_no_escape.html', escape=False)\n",
    "print(\"Example 12: Saved to 'iris_no_escape.html' with HTML escaping disabled\\n\")\n",
    "\n",
    "# üîπ Example 13: Rendering links (render_links)\n",
    "df.to_html('iris_links.html', render_links=True, escape=False)\n",
    "print(\"Example 13: Rendering to 'iris_links.html' with clickable links\\n\")\n",
    "\n",
    "# Verify one of the HTML files by reading it back\n",
    "print(\"Sample content of 'iris_basic.html' (first 5 rows):\")\n",
    "df_read = pd.read_html('iris_basic.html')[0]\n",
    "print(df_read.head(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aca637f",
   "metadata": {},
   "source": [
    "# üìä Converting DataFrames to JSON in Pandas\n",
    "\n",
    "## üì§ Writing DataFrames to JSON with `to_json()`\n",
    "\n",
    "The `pandas.DataFrame.to_json()` method converts a Pandas DataFrame to a JSON (JavaScript Object Notation) string or file, a lightweight format widely used for data interchange in APIs, web applications, and data storage. This method offers flexible options for JSON structure, formatting, and content customization. This guide explains the most useful and frequently used parameters of `to_json()` with examples using a real-world dataset, focusing on practical applications for JSON export.\n",
    "\n",
    "### üîπ Dataset\n",
    "We‚Äôll use the **Iris Dataset**, which contains measurements of iris flowers, including columns like `sepal_length`, `sepal_width`, `petal_length`, `petal_width`, and `species`. The dataset is available as a tab-separated text file at:\n",
    "- URL: `https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.txt`\n",
    "- Process: We‚Äôll load the dataset into a DataFrame and convert it to JSON files to demonstrate `to_json()` parameters.\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Most Useful `to_json()` Parameters\n",
    "\n",
    "Below are the most useful and frequently used parameters for `pandas.DataFrame.to_json()`, along with their descriptions and use cases.\n",
    "\n",
    "| Parameter          | Description                                              |\n",
    "|--------------------|----------------------------------------------------------|\n",
    "| `path_or_buf`     | File path or file-like object to write the JSON output. If `None`, returns the JSON as a string. Default is `None`. |\n",
    "| `orient`          | JSON structure format: `'split'`, `'records'`, `'index'`, `'columns'`, `'values'`, or `'table'`. Default is `'columns'`. Determines how the DataFrame is represented in JSON. |\n",
    "| `index`           | If `True`, includes the DataFrame index in the JSON output. Default is `True`. Set to `False` to exclude the index (behavior depends on `orient`). |\n",
    "| `date_format`     | Format for datetime columns: `'epoch'` (Unix timestamp) or `'iso'` (ISO8601). Default is `'epoch'` for `'records'` orient, `'iso'` otherwise. Useful for date serialization. |\n",
    "| `double_precision`| Number of decimal places for floating-point numbers (0-15). Default is `10`. Controls numerical precision. |\n",
    "| `force_ascii`     | If `True`, encodes all characters as ASCII. Default is `True`. Set to `False` to preserve non-ASCII characters (e.g., Unicode). |\n",
    "| `lines`           | If `True`, writes each row as a separate JSON object (JSON Lines format). Default is `False`. Useful for streaming or large datasets. Requires `orient='records'`. |\n",
    "| `indent`          | Number of spaces for JSON indentation. Default is `None` (compact output). Set to an integer (e.g., `2`) for pretty-printed JSON. |\n",
    "| `compression`     | Compression method for the output file (e.g., `'gzip'`, `'bz2'`, `'zip'`, `'xz'`). Default is `'infer'` (based on file extension). Saves disk space for large files. |\n",
    "| `date_unit`       | Time unit for datetime columns: `'s'` (seconds), `'ms'` (milliseconds), `'us'` (microseconds), or `'ns'` (nanoseconds). Default is `'ms'`. Relevant for `'epoch'` date format. |\n",
    "\n",
    "For a complete list of parameters, refer to the [official pandas documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_json.html).\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Examples\n",
    "\n",
    "Below are examples of how to use each parameter with the Iris dataset, converting the DataFrame to JSON files or strings with various configurations. These are implemented in a separate Python script (`to_json_pandas_examples.py`).\n",
    "\n",
    "1. **Basic Usage (`path_or_buf`)**: Converts the DataFrame to JSON and saves it to a file or returns as a string.\n",
    "2. **Specifying Orientation (`orient`)**: Uses different JSON structures (e.g., `'records'`, `'split'`, `'table'`).\n",
    "3. **Excluding Index (`index`)**: Generates JSON without the DataFrame index.\n",
    "4. **Formatting Dates (`date_format`)**: Controls datetime serialization (demonstrated with a modified dataset).\n",
    "5. **Controlling Float Precision (`double_precision`)**: Limits decimal places for floating-point numbers.\n",
    "6. **Preserving Non-ASCII Characters (`force_ascii`)**: Allows Unicode characters in the output (demonstrated with modified data).\n",
    "7. **Writing JSON Lines (`lines`)**: Outputs each row as a separate JSON object.\n",
    "8. **Pretty-Printing JSON (`indent`)**: Formats JSON with indentation for readability.\n",
    "9. **Applying Compression (`compression`)**: Saves the JSON as a compressed file (e.g., `.gz`).\n",
    "10. **Customizing Date Units (`date_unit`)**: Specifies time units for datetime columns (demonstrated with a modified dataset).\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Most Popular Parameters\n",
    "\n",
    "The following `to_json()` parameters are the most commonly used in real-world JSON export tasks:\n",
    "- **`path_or_buf`**: Frequently used to save JSON to a file or capture it as a string for API responses.\n",
    "- **`orient`**: Critical for matching the JSON structure to the target application (e.g., `'records'` for API data, `'table'` for schema preservation).\n",
    "- **`index`**: Often set to `False` to exclude the index, especially for datasets without meaningful indices.\n",
    "- **`date_format`**: Essential for consistent datetime serialization in web applications or APIs.\n",
    "- **`lines`**: Widely used for JSON Lines format in streaming or log-based applications.\n",
    "- **`indent`**: Useful for human-readable JSON during development or debugging.\n",
    "- **`compression`**: Saves disk space and bandwidth for large JSON files.\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Notes\n",
    "- **Dependencies**: Requires `pandas`. Install with:\n",
    "  ```bash\n",
    "  pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc0fa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### üìù Python Script: `to_json_pandas_examples.py`\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the Iris dataset\n",
    "url = 'https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.txt'\n",
    "df = pd.read_csv(url, sep='\\t')\n",
    "\n",
    "# Add a datetime column for demonstrating date_format and date_unit\n",
    "df['date_added'] = pd.date_range(start='2025-01-01', periods=len(df), freq='D')\n",
    "\n",
    "# Add some missing values for demonstration\n",
    "df.loc[0:2, 'sepal_length'] = None\n",
    "\n",
    "# Add a column with non-ASCII characters for demonstrating force_ascii\n",
    "df['species_unicode'] = df['species'].apply(lambda x: f'{x} (Ëä±)')\n",
    "\n",
    "# üîπ Example 1: Basic Usage (path_or_buf)\n",
    "# Save to file\n",
    "df.to_json('iris_basic.json')\n",
    "print(\"Example 1a: Saved to 'iris_basic.json' with default settings\")\n",
    "# Return as string\n",
    "json_str = df.to_json()\n",
    "print(\"Example 1b: JSON string (first 200 characters):\")\n",
    "print(json_str[:200], '...\\n')\n",
    "\n",
    "# üîπ Example 2: Specifying orientation (orient)\n",
    "# Records orientation\n",
    "df.to_json('iris_records.json', orient='records')\n",
    "print(\"Example 2a: Saved to 'iris_records.json' with orient='records'\")\n",
    "# Split orientation\n",
    "df.to_json('iris_split.json', orient='split')\n",
    "print(\"Example 2b: Saved to 'iris_split.json' with orient='split'\")\n",
    "# Table orientation\n",
    "df.to_json('iris_table.json', orient='table')\n",
    "print(\"Example 2c: Saved to 'iris_table.json' with orient='table'\\n\")\n",
    "\n",
    "# üîπ Example 3: Excluding index (index)\n",
    "df.to_json('iris_no_index.json', index=False, orient='records')\n",
    "print(\"Example 3: Saved to 'iris_no_index.json' without index (orient='records')\\n\")\n",
    "\n",
    "# üîπ Example 4: Formatting dates (date_format)\n",
    "df.to_json('iris_date_format.json', date_format='iso', orient='records')\n",
    "print(\"Example 4: Saved to 'iris_date_format.json' with ISO date format\\n\")\n",
    "\n",
    "# üîπ Example 5: Controlling float precision (double_precision)\n",
    "df.to_json('iris_precision.json', double_precision=2, orient='records')\n",
    "print(\"Example 5: Saved to 'iris_precision.json' with 2 decimal places\\n\")\n",
    "\n",
    "# üîπ Example 6: Preserving non-ASCII characters (force_ascii)\n",
    "df.to_json('iris_unicode.json', force_ascii=False, orient='records')\n",
    "print(\"Example 6: Saved to 'iris_unicode.json' with non-ASCII characters preserved\\n\")\n",
    "\n",
    "# üîπ Example 7: Writing JSON Lines (lines)\n",
    "df.to_json('iris_lines.json', orient='records', lines=True)\n",
    "print(\"Example 7: Saved to 'iris_lines.json' in JSON Lines format\\n\")\n",
    "\n",
    "# üîπ Example 8: Pretty-printing JSON (indent)\n",
    "df.to_json('iris_pretty.json', indent=2, orient='records')\n",
    "print(\"Example 8: Saved to 'iris_pretty.json' with indentation\\n\")\n",
    "\n",
    "# üîπ Example 9: Applying compression (compression)\n",
    "df.to_json('iris_compressed.json.gz', compression='gzip', orient='records')\n",
    "print(\"Example 9: Saved to 'iris_compressed.json.gz' with gzip compression\\n\")\n",
    "\n",
    "# üîπ Example 10: Customizing date units (date_unit)\n",
    "df.to_json('iris_date_unit.json', date_format='epoch', date_unit='s', orient='records')\n",
    "print(\"Example 10: Saved to 'iris_date_unit.json' with epoch seconds\\n\")\n",
    "\n",
    "# Verify one of the JSON files by reading it back\n",
    "print(\"Sample content of 'iris_records.json' (first 5 rows):\")\n",
    "df_read = pd.read_json('iris_records.json', orient='records')\n",
    "print(df_read.head(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f4e544",
   "metadata": {},
   "source": [
    "# üìä Saving DataFrames to MySQL Databases in Pandas with SQLAlchemy\n",
    "\n",
    "## üì§ Writing DataFrames to MySQL with `to_sql()` and SQLAlchemy\n",
    "\n",
    "The `pandas.DataFrame.to_sql()` method saves a Pandas DataFrame to a SQL database table, enabling seamless integration with relational databases like MySQL. When paired with **SQLAlchemy** and the **PyMySQL** driver, it provides a robust and flexible way to connect to MySQL databases. This guide explains the most useful parameters of `to_sql()` with a focus on using SQLAlchemy with PyMySQL, including the specific connection setup `create_engine(\"mysql+pymysql://root:@localhost/ipl\")`. Examples use a real-world dataset for practical application.\n",
    "\n",
    "### üîπ Dataset\n",
    "We‚Äôll use the **IPL Deliveries Dataset**, which contains ball-by-ball data from Indian Premier League cricket matches, including columns like `match_id`, `inning`, `batting_team`, `bowling_team`, `over`, `ball`, `batsman`, `bowler`, `runs`, etc. The dataset is available as a CSV file at:\n",
    "- URL: `https://raw.githubusercontent.com/datacamp/courses-introduction-to-python/master/datasets/ipl_deliveries.csv`\n",
    "- Process: We‚Äôll load the dataset into a DataFrame and save it to a MySQL table named `ipl_Delivery` in the `ipl` database.\n",
    "\n",
    "### üîπ Setting Up MySQL with XAMPP\n",
    "\n",
    "**XAMPP** is a free, open-source software stack that includes Apache, MySQL, PHP, and phpMyAdmin, ideal for setting up a local MySQL server.\n",
    "\n",
    "#### Installation Steps\n",
    "1. **Download XAMPP**:\n",
    "   - Visit [Apache Friends](https://www.apachefriends.org/index.html) and download XAMPP for your operating system.\n",
    "   - Verified as accessible on June 4, 2025, at 3:48 PM IST.\n",
    "2. **Install XAMPP**:\n",
    "   - Run the installer, selecting Apache, MySQL, and phpMyAdmin.\n",
    "   - Install to a default location (e.g., `C:\\xampp` on Windows).\n",
    "3. **Start MySQL**:\n",
    "   - Open the XAMPP Control Panel and start **Apache** and **MySQL** modules.\n",
    "   - MySQL runs on port `3306` by default.\n",
    "4. **Access phpMyAdmin**:\n",
    "   - Navigate to `http://localhost/phpmyadmin` in a browser.\n",
    "   - Log in with default credentials (username: `root`, password: empty unless changed).\n",
    "5. **Create the Database**:\n",
    "   - In phpMyAdmin, click **New** and create a database named `ipl`.\n",
    "   - No tables are needed initially, as `to_sql()` will create the `ipl_Delivery` table.\n",
    "\n",
    "#### XAMPP Configuration Notes\n",
    "- **Default Credentials**: Username is `root`, password is empty (`\"\"`). Update the connection string if you set a password (e.g., `mysql+pymysql://root:password@localhost/ipl`).\n",
    "- **Port Conflicts**: Ensure port `3306` is free. Check XAMPP Control Panel or MySQL logs if MySQL fails to start.\n",
    "- **Security**: For production, set a strong password and restrict database access with a firewall.\n",
    "\n",
    "### üîπ Connecting to MySQL with SQLAlchemy and PyMySQL\n",
    "\n",
    "**SQLAlchemy** is a Python SQL toolkit that provides a unified interface for database connections, and **PyMySQL** is a pure Python MySQL driver used with SQLAlchemy.\n",
    "\n",
    "#### Installation\n",
    "Install the required libraries:\n",
    "```bash\n",
    "pip install pandas sqlalchemy pymysql"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843cac1c",
   "metadata": {},
   "source": [
    "#### `mysql+pymysql://{user}:{password}@{host}/{database}`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccf6521",
   "metadata": {},
   "source": [
    "#### `from sqlalchemy import create_engine`\n",
    "#### `engine = create_engine(\"mysql+pymysql://root:@localhost/ipl\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d890c7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### üìù Python Script: `to_sql_sqlalchemy_pandas_examples.py`\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import pymysql\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "\n",
    "# Load the IPL Deliveries dataset\n",
    "url = 'https://raw.githubusercontent.com/datacamp/courses-introduction-to-python/master/datasets/ipl_deliveries.csv'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Add some missing values for demonstration\n",
    "df.loc[0:2, 'total_runs'] = None\n",
    "\n",
    "# Create SQLAlchemy engine\n",
    "try:\n",
    "    engine = create_engine(\"mysql+pymysql://root:@localhost/ipl\")\n",
    "\n",
    "    # üîπ Example 1: Basic Usage (name, con)\n",
    "    df.to_sql(name='ipl_Delivery', con=engine, if_exists='replace')\n",
    "    print(\"Example 1: Saved DataFrame to 'ipl_Delivery' table\")\n",
    "    # Verify by reading back\n",
    "    df_read = pd.read_sql_query('SELECT * FROM ipl_Delivery LIMIT 5', engine)\n",
    "    print(df_read, '\\n')\n",
    "\n",
    "    # üîπ Example 2: Handling existing tables (if_exists)\n",
    "    # Append a subset of the DataFrame\n",
    "    df_subset = df.head(10)\n",
    "    df_subset.to_sql(name='ipl_Delivery', con=engine, if_exists='append')\n",
    "    print(\"Example 2: Appended 10 rows to 'ipl_Delivery' table (if_exists='append')\")\n",
    "    # Verify row count\n",
    "    count = pd.read_sql_query('SELECT COUNT(*) AS count FROM ipl_Delivery', engine)['count'].iloc[0]\n",
    "    print(f\"Total rows in 'ipl_Delivery' table: {count}\\n\")\n",
    "\n",
    "    # üîπ Example 3: Excluding index (index)\n",
    "    df.to_sql(name='ipl_Delivery_no_index', con=engine, if_exists='replace', index=False)\n",
    "    print(\"Example 3: Saved to 'ipl_Delivery_no_index' table without index\")\n",
    "    # Verify columns\n",
    "    df_read_no_index = pd.read_sql_query('SELECT * FROM ipl_Delivery_no_index LIMIT 5', engine)\n",
    "    print(df_read_no_index, '\\n')\n",
    "\n",
    "    # üîπ Example 4: Custom index label (index_label)\n",
    "    df.to_sql(name='ipl_Delivery_custom_index', con=engine, if_exists='replace', index=True, index_label='record_id')\n",
    "    print(\"Example 4: Saved to 'ipl_Delivery_custom_index' table with index labeled 'record_id'\")\n",
    "    # Verify index column\n",
    "    df_read_custom_index = pd.read_sql_query('SELECT record_id, match_id, total_runs FROM ipl_Delivery_custom_index LIMIT 5', engine)\n",
    "    print(df_read_custom_index, '\\n')\n",
    "\n",
    "    # üîπ Example 5: Specifying data types (dtype)\n",
    "    dtype_dict = {\n",
    "        'match_id': 'INT',\n",
    "        'inning': 'INT',\n",
    "        'over': 'INT',\n",
    "        'ball': 'INT',\n",
    "        'total_runs': 'FLOAT',\n",
    "        'batting_team': 'VARCHAR(100)',\n",
    "        'bowling_team': 'VARCHAR(100)',\n",
    "        'batsman': 'VARCHAR(100)',\n",
    "        'bowler': 'VARCHAR(100)'\n",
    "    }\n",
    "    df.to_sql(name='ipl_Delivery_dtype', con=engine, if_exists='replace', dtype=dtype_dict)\n",
    "    print(\"Example 5: Saved to 'ipl_Delivery_dtype' table with custom SQL data types\")\n",
    "    # Verify schema\n",
    "    schema = pd.read_sql_query(\"SHOW COLUMNS FROM ipl_Delivery_dtype\", engine)\n",
    "    print(schema, '\\n')\n",
    "\n",
    "    # üîπ Example 6: Writing in chunks (chunksize)\n",
    "    df.to_sql(name='ipl_Delivery_chunks', con=engine, if_exists='replace', chunksize=10000)\n",
    "    print(\"Example 6: Saved to 'ipl_Delivery_chunks' table in chunks of 10,000 rows\")\n",
    "    # Verify\n",
    "    df_read_chunks = pd.read_sql_query('SELECT * FROM ipl_Delivery_chunks LIMIT 5', engine)\n",
    "    print(df_read_chunks, '\\n')\n",
    "\n",
    "    # üîπ Example 7: Using multi-row insert (method='multi')\n",
    "    df.to_sql(name='ipl_Delivery_multi', con=engine, if_exists='replace', method='multi')\n",
    "    print(\"Example 7: Saved to 'ipl_Delivery_multi' table using multi-row insert\")\n",
    "    # Verify\n",
    "    df_read_multi = pd.read_sql_query('SELECT * FROM ipl_Delivery_multi LIMIT 5', engine)\n",
    "    print(df_read_multi, '\\n')\n",
    "\n",
    "except SQLAlchemyError as e:\n",
    "    print(f\"Error interacting with MySQL: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Dispose of the engine\n",
    "    if 'engine' in locals():\n",
    "        engine.dispose()\n",
    "        print(\"SQLAlchemy engine disposed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
