{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56d87b5d",
   "metadata": {},
   "source": [
    "# Selenium Web Scraping Guide for Python\n",
    "\n",
    "## What is Selenium?\n",
    "\n",
    "Selenium is a powerful automation framework primarily used for testing web applications, but it's also widely used for web scraping. Unlike traditional scraping libraries like BeautifulSoup or requests, Selenium can interact with JavaScript-heavy websites by controlling a real web browser programmatically.\n",
    "\n",
    "### Key Features:\n",
    "- **Browser Automation**: Controls real browsers (Chrome, Firefox, Safari, Edge)\n",
    "- **JavaScript Support**: Can scrape dynamic content loaded by JavaScript\n",
    "- **User Interaction**: Can click buttons, fill forms, scroll pages, and simulate user behavior\n",
    "- **Cross-Platform**: Works on Windows, macOS, and Linux\n",
    "- **Multiple Language Support**: Available for Python, Java, C#, Ruby, and JavaScript\n",
    "\n",
    "## Installation\n",
    "\n",
    "### 1. Install Selenium Package\n",
    "```bash\n",
    "pip install selenium\n",
    "```\n",
    "\n",
    "### 2. Install WebDriver Manager (Recommended)\n",
    "```bash\n",
    "pip install webdriver-manager\n",
    "```\n",
    "\n",
    "### 3. Alternative: Manual WebDriver Installation\n",
    "If you prefer manual installation, download the appropriate driver:\n",
    "- **Chrome**: [ChromeDriver](https://chromedriver.chromium.org/)\n",
    "- **Firefox**: [GeckoDriver](https://github.com/mozilla/geckodriver/releases)\n",
    "- **Edge**: [EdgeDriver](https://developer.microsoft.com/en-us/microsoft-edge/tools/webdriver/)\n",
    "\n",
    "## Imports and Basic Setup\n",
    "\n",
    "### Essential Imports\n",
    "```python\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "```\n",
    "\n",
    "### WebDriver Setup Options\n",
    "\n",
    "#### Option 1: Using WebDriver Manager (Recommended)\n",
    "```python\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "# Automatically downloads and manages ChromeDriver\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service)\n",
    "```\n",
    "\n",
    "#### Option 2: Manual WebDriver Path\n",
    "```python\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "# Specify the path to your downloaded ChromeDriver\n",
    "service = Service('/path/to/chromedriver')\n",
    "driver = webdriver.Chrome(service=service)\n",
    "```\n",
    "\n",
    "#### Option 3: Chrome Options for Customization\n",
    "```python\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--headless')  # Run in background\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "chrome_options.add_argument('--window-size=1920,1080')\n",
    "\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "```\n",
    "\n",
    "## Step-by-Step Web Scraping Process\n",
    "\n",
    "### Step 1: Initialize WebDriver\n",
    "```python\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "# Set up the driver\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service)\n",
    "```\n",
    "\n",
    "### Step 2: Navigate to Website\n",
    "```python\n",
    "# Open the target website\n",
    "url = \"https://example.com\"\n",
    "driver.get(url)\n",
    "\n",
    "# Optional: Maximize window\n",
    "driver.maximize_window()\n",
    "```\n",
    "\n",
    "### Step 3: Locate Elements\n",
    "Selenium provides multiple ways to find elements:\n",
    "\n",
    "```python\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# By ID\n",
    "element = driver.find_element(By.ID, \"element-id\")\n",
    "\n",
    "# By Class Name\n",
    "element = driver.find_element(By.CLASS_NAME, \"class-name\")\n",
    "\n",
    "# By Tag Name\n",
    "element = driver.find_element(By.TAG_NAME, \"div\")\n",
    "\n",
    "# By XPath\n",
    "element = driver.find_element(By.XPATH, \"//div[@class='example']\")\n",
    "\n",
    "# By CSS Selector\n",
    "element = driver.find_element(By.CSS_SELECTOR, \".class-name\")\n",
    "\n",
    "# Find multiple elements\n",
    "elements = driver.find_elements(By.CLASS_NAME, \"multiple-class\")\n",
    "```\n",
    "\n",
    "### Step 4: Extract Data\n",
    "```python\n",
    "# Get text content\n",
    "text = element.text\n",
    "\n",
    "# Get attribute values\n",
    "href = element.get_attribute(\"href\")\n",
    "src = element.get_attribute(\"src\")\n",
    "\n",
    "# Get HTML content\n",
    "html = element.get_attribute(\"innerHTML\")\n",
    "```\n",
    "\n",
    "### Step 5: Handle Dynamic Content\n",
    "```python\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Wait for element to be present\n",
    "wait = WebDriverWait(driver, 10)\n",
    "element = wait.until(EC.presence_of_element_located((By.ID, \"dynamic-content\")))\n",
    "\n",
    "# Wait for element to be clickable\n",
    "clickable_element = wait.until(EC.element_to_be_clickable((By.ID, \"button-id\")))\n",
    "```\n",
    "\n",
    "### Step 6: Interact with Elements\n",
    "```python\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "# Click elements\n",
    "element.click()\n",
    "\n",
    "# Send text to input fields\n",
    "input_field.send_keys(\"your text here\")\n",
    "\n",
    "# Clear input fields\n",
    "input_field.clear()\n",
    "\n",
    "# Submit forms\n",
    "form.submit()\n",
    "\n",
    "# Scroll page\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "# Press keyboard keys\n",
    "element.send_keys(Keys.ENTER)\n",
    "element.send_keys(Keys.TAB)\n",
    "```\n",
    "\n",
    "### Step 7: Handle Multiple Pages/Pagination\n",
    "```python\n",
    "# Example: Scraping multiple pages\n",
    "page_num = 1\n",
    "all_data = []\n",
    "\n",
    "while True:\n",
    "    # Scrape current page\n",
    "    elements = driver.find_elements(By.CLASS_NAME, \"data-item\")\n",
    "    \n",
    "    for element in elements:\n",
    "        data = element.text\n",
    "        all_data.append(data)\n",
    "    \n",
    "    # Try to find \"Next\" button\n",
    "    try:\n",
    "        next_button = driver.find_element(By.XPATH, \"//a[contains(text(), 'Next')]\")\n",
    "        next_button.click()\n",
    "        time.sleep(2)  # Wait for page to load\n",
    "        page_num += 1\n",
    "    except:\n",
    "        print(f\"Scraped {page_num} pages\")\n",
    "        break\n",
    "```\n",
    "\n",
    "### Step 8: Clean Up\n",
    "```python\n",
    "# Always close the driver when done\n",
    "driver.quit()\n",
    "```\n",
    "\n",
    "## Complete Example: Scraping a News Website\n",
    "\n",
    "```python\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import time\n",
    "import csv\n",
    "\n",
    "def scrape_news_website():\n",
    "    # Setup\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service)\n",
    "    \n",
    "    try:\n",
    "        # Navigate to website\n",
    "        driver.get(\"https://news.ycombinator.com\")\n",
    "        \n",
    "        # Wait for content to load\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        articles = wait.until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"titleline\")))\n",
    "        \n",
    "        # Extract data\n",
    "        news_data = []\n",
    "        for article in articles[:10]:  # Get first 10 articles\n",
    "            try:\n",
    "                title_element = article.find_element(By.TAG_NAME, \"a\")\n",
    "                title = title_element.text\n",
    "                link = title_element.get_attribute(\"href\")\n",
    "                \n",
    "                news_data.append({\n",
    "                    'title': title,\n",
    "                    'link': link\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting article: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Save to CSV\n",
    "        with open('news_data.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=['title', 'link'])\n",
    "            writer.writeheader()\n",
    "            writer.writerows(news_data)\n",
    "        \n",
    "        print(f\"Successfully scraped {len(news_data)} articles\")\n",
    "        return news_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# Run the scraper\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_news_website()\n",
    "```\n",
    "\n",
    "## Best Practices and Tips\n",
    "\n",
    "### 1. Respect Robots.txt and Rate Limiting\n",
    "```python\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Add delays between requests\n",
    "time.sleep(random.uniform(1, 3))\n",
    "```\n",
    "\n",
    "### 2. Handle Exceptions\n",
    "```python\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "\n",
    "try:\n",
    "    element = driver.find_element(By.ID, \"some-id\")\n",
    "except NoSuchElementException:\n",
    "    print(\"Element not found\")\n",
    "except TimeoutException:\n",
    "    print(\"Page load timeout\")\n",
    "```\n",
    "\n",
    "### 3. Use Headless Mode for Production\n",
    "```python\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "options = Options()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "```\n",
    "\n",
    "### 4. Implement Retry Logic\n",
    "```python\n",
    "def retry_find_element(driver, by, value, max_attempts=3):\n",
    "    for attempt in range(max_attempts):\n",
    "        try:\n",
    "            return driver.find_element(by, value)\n",
    "        except NoSuchElementException:\n",
    "            if attempt == max_attempts - 1:\n",
    "                raise\n",
    "            time.sleep(1)\n",
    "```\n",
    "\n",
    "### 5. Use Context Managers\n",
    "```python\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def get_driver():\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service)\n",
    "    try:\n",
    "        yield driver\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# Usage\n",
    "with get_driver() as driver:\n",
    "    driver.get(\"https://example.com\")\n",
    "    # Your scraping code here\n",
    "```\n",
    "\n",
    "## Common Challenges and Solutions\n",
    "\n",
    "### 1. CAPTCHA and Bot Detection\n",
    "- Use random delays\n",
    "- Rotate user agents\n",
    "- Use proxy servers\n",
    "- Implement human-like behavior patterns\n",
    "\n",
    "### 2. Dynamic Content Loading\n",
    "- Use WebDriverWait with expected conditions\n",
    "- Implement scroll-based loading detection\n",
    "- Monitor network activity\n",
    "\n",
    "### 3. Session Management\n",
    "- Handle cookies and sessions\n",
    "- Implement login flows\n",
    "- Maintain session state across pages\n",
    "\n",
    "### 4. Performance Optimization\n",
    "- Use headless mode\n",
    "- Disable images and CSS when not needed\n",
    "- Implement parallel processing with multiple drivers\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Selenium is a powerful tool for web scraping, especially for JavaScript-heavy websites. While it's slower than traditional HTTP-based scraping methods, its ability to interact with dynamic content makes it invaluable for many scraping tasks. Always remember to scrape responsibly and respect website terms of service and robots.txt files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77b517cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Ajio Scraper with Slow Scrolling\n",
      "Navigating to https://www.ajio.com/men-shirts/c/830216013\n",
      "Error waiting for page load: Message: invalid session id\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0x7ff60fc56f35+78965]\n",
      "\tGetHandleVerifier [0x0x7ff60fc56f90+79056]\n",
      "\t(No symbol) [0x0x7ff60f9e9c0c]\n",
      "\t(No symbol) [0x0x7ff60fa3043f]\n",
      "\t(No symbol) [0x0x7ff60fa68532]\n",
      "\t(No symbol) [0x0x7ff60fa62f5c]\n",
      "\t(No symbol) [0x0x7ff60fa62039]\n",
      "\t(No symbol) [0x0x7ff60f9b5fc5]\n",
      "\tGetHandleVerifier [0x0x7ff60ff0e23d+2926461]\n",
      "\tGetHandleVerifier [0x0x7ff60ff08963+2903715]\n",
      "\tGetHandleVerifier [0x0x7ff60ff26abd+3026941]\n",
      "\tGetHandleVerifier [0x0x7ff60fc716ce+187406]\n",
      "\tGetHandleVerifier [0x0x7ff60fc796bf+220159]\n",
      "\t(No symbol) [0x0x7ff60f9b5036]\n",
      "\tGetHandleVerifier [0x0x7ff6100172c8+4012040]\n",
      "\tBaseThreadInitThunk [0x0x7ffbbcd6e8d7+23]\n",
      "\tRtlUserThreadStart [0x0x7ffbbe43c34c+44]\n",
      "\n",
      "Starting slow incremental scrolling...\n",
      "An error occurred: Message: invalid session id\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0x7ff60fc56f35+78965]\n",
      "\tGetHandleVerifier [0x0x7ff60fc56f90+79056]\n",
      "\t(No symbol) [0x0x7ff60f9e9c0c]\n",
      "\t(No symbol) [0x0x7ff60fa3043f]\n",
      "\t(No symbol) [0x0x7ff60fa68532]\n",
      "\t(No symbol) [0x0x7ff60fa62f5c]\n",
      "\t(No symbol) [0x0x7ff60fa62039]\n",
      "\t(No symbol) [0x0x7ff60f9b5fc5]\n",
      "\tGetHandleVerifier [0x0x7ff60ff0e23d+2926461]\n",
      "\tGetHandleVerifier [0x0x7ff60ff08963+2903715]\n",
      "\tGetHandleVerifier [0x0x7ff60ff26abd+3026941]\n",
      "\tGetHandleVerifier [0x0x7ff60fc716ce+187406]\n",
      "\tGetHandleVerifier [0x0x7ff60fc796bf+220159]\n",
      "\t(No symbol) [0x0x7ff60f9b5036]\n",
      "\tGetHandleVerifier [0x0x7ff6100172c8+4012040]\n",
      "\tBaseThreadInitThunk [0x0x7ffbbcd6e8d7+23]\n",
      "\tRtlUserThreadStart [0x0x7ffbbe43c34c+44]\n",
      "\n",
      "Browser closed\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "def initialize_driver(chromedriver_path):\n",
    "    \"\"\"Initialize Chrome WebDriver with options.\"\"\"\n",
    "    try:\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument('--disable-blink-features=AutomationControlled')  # Avoid bot detection\n",
    "        service = Service(chromedriver_path)\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        driver.maximize_window()  # Maximize window to ensure all content is visible\n",
    "        return driver\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing driver: {e}\")\n",
    "        raise\n",
    "\n",
    "def wait_for_page_load(driver):\n",
    "    \"\"\"Wait for initial page content to load with extended timeout.\"\"\"\n",
    "    try:\n",
    "        WebDriverWait(driver, 30).until(  # Increased timeout for slow connections\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"div[class*='items']\"))\n",
    "        )\n",
    "        time.sleep(random.uniform(3, 6))  # Longer delay for initial load\n",
    "        print(\"Initial page content loaded\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error waiting for page load: {e}\")\n",
    "\n",
    "def slow_incremental_scroll(driver):\n",
    "    \"\"\"\n",
    "    Slowly scroll the page in small increments with longer delays\n",
    "    to accommodate slow internet connections.\n",
    "    \"\"\"\n",
    "    print(\"Starting slow incremental scrolling...\")\n",
    "    \n",
    "    # Initial values\n",
    "    scroll_step = 300  # Small scroll increment\n",
    "    scroll_position = 0\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    no_change_count = 0\n",
    "    max_no_change = 5\n",
    "    max_scrolls = 500\n",
    "    scroll_count = 0\n",
    "    \n",
    "    while no_change_count < max_no_change and scroll_count < max_scrolls:\n",
    "        scroll_count += 1\n",
    "        \n",
    "        # Calculate next scroll position\n",
    "        scroll_position += scroll_step\n",
    "        if scroll_position > last_height:\n",
    "            scroll_position = last_height  # Don't scroll beyond current height\n",
    "        \n",
    "        # Scroll to position\n",
    "        driver.execute_script(f\"window.scrollTo(0, {scroll_position});\")\n",
    "        \n",
    "        # Extended delay for slow connections\n",
    "        time.sleep(random.uniform(3, 6))  # Longer delay between scrolls\n",
    "        \n",
    "        # Check for \"Load More\" button\n",
    "        try:\n",
    "            load_button = driver.find_element(By.XPATH, \"//button[contains(., 'Load More')]\")\n",
    "            driver.execute_script(\"arguments[0].click();\", load_button)\n",
    "            print(\"Clicked 'Load More' button\")\n",
    "            time.sleep(random.uniform(5, 8))  # Extra long delay after clicking\n",
    "            # Reset scroll position since new content loaded\n",
    "            scroll_position = driver.execute_script(\"return window.pageYOffset\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Get new document height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "        # Update scroll position if content has expanded\n",
    "        if new_height > last_height:\n",
    "            last_height = new_height\n",
    "            no_change_count = 0  # Reset no-change counter\n",
    "        else:\n",
    "            # If we're at the bottom and no new content\n",
    "            if scroll_position >= last_height:\n",
    "                no_change_count += 1\n",
    "        \n",
    "        print(f\"Scroll: {scroll_count} | Position: {scroll_position}/{last_height} | \"\n",
    "              f\"No-change: {no_change_count}/{max_no_change}\")\n",
    "        \n",
    "        # Add random pauses to mimic human behavior\n",
    "        if random.random() < 0.3:  # 30% chance of longer pause\n",
    "            pause_time = random.uniform(8, 15)\n",
    "            print(f\"Long pause: {pause_time:.1f} seconds\")\n",
    "            time.sleep(pause_time)\n",
    "    \n",
    "    if no_change_count >= max_no_change:\n",
    "        print(f\"Stopped after {max_no_change} consecutive no-change events\")\n",
    "    elif scroll_count >= max_scrolls:\n",
    "        print(f\"Reached maximum scroll attempts ({max_scrolls})\")\n",
    "    else:\n",
    "        print(\"Scroll completed successfully\")\n",
    "\n",
    "def save_html(driver, output_file):\n",
    "    \"\"\"Save HTML content to file.\"\"\"\n",
    "    try:\n",
    "        # Scroll to top to ensure all elements are in DOM\n",
    "        driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "        time.sleep(3)  # Longer delay before saving\n",
    "        \n",
    "        html = driver.page_source\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(html)\n",
    "        print(f\"HTML content saved to {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving HTML: {e}\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to execute web scraping with slow scrolling.\"\"\"\n",
    "    # Configuration\n",
    "    chromedriver_path = 'C:/Users/bhaut/OneDrive/Desktop/chromedriver.exe'\n",
    "    url = 'https://www.ajio.com/men-shirts/c/830216013'\n",
    "    output_file = 'ajio_full.html'\n",
    "    \n",
    "    # Initialize driver\n",
    "    driver = initialize_driver(chromedriver_path)\n",
    "    \n",
    "    try:\n",
    "        # Load webpage\n",
    "        print(f\"Navigating to {url}\")\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Handle cookie consent if present\n",
    "        try:\n",
    "            WebDriverWait(driver, 10).until(  # Longer timeout for cookie consent\n",
    "                EC.element_to_be_clickable((By.ID, \"allow-button\"))\n",
    "            ).click()\n",
    "            print(\"Cookies accepted\")\n",
    "            time.sleep(3)  # Pause after accepting cookies\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Wait for initial load with extended timeout\n",
    "        wait_for_page_load(driver)\n",
    "        \n",
    "        # Continuously scroll to load all content slowly\n",
    "        slow_incremental_scroll(driver)\n",
    "        \n",
    "        # Save HTML\n",
    "        save_html(driver, output_file)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        # Save partial HTML for debugging\n",
    "        try:\n",
    "            with open('partial_ajio.html', 'w', encoding='utf-8') as f:\n",
    "                f.write(driver.page_source)\n",
    "            print(\"Partial HTML saved to partial_ajio.html\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    finally:\n",
    "        # Clean up\n",
    "        driver.quit()\n",
    "        print(\"Browser closed\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting Ajio Scraper with Slow Scrolling\")\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
